{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Melanoma analysis with Inception ResNet V2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook shows how good is [Inception ResNet V2](#Inception-ResNet-V2) for [melanoma](#Melanoma) analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_curve, auc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check if a GPU is available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.config.list_physical_devices('GPU')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Melanoma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Melanoma__, also redundantly known as __malignant melanoma__, is a type of skin cancer that develops from the pigment-producing cells known as melanocytes. Melanomas typically occur in the skin, but may rarely occur in the mouth, intestines, or eye (uveal melanoma). In women, they most commonly occur on the legs, while in men, they most commonly occur on the back. About 25% of melanomas develop from moles. Changes in a mole that can indicate melanoma include an increase in size, irregular edges, change in color, itchiness, or skin breakdown."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![melanoma image](../assets/melanoma.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center; font-weight: bold\">Pic.1. A melanoma of approximately 2.5 cm (1 in) by 1.5 cm (0.6 in)</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The primary cause of melanoma is ultraviolet light (UV) exposure in those with low levels of the skin pigment melanin. The UV light may be from the sun or other sources, such as tanning devices. Those with many moles, a history of affected family members, and poor immune function are at greater risk. A number of rare genetic conditions, such as xeroderma pigmentosum, also increase the risk. Diagnosis is by biopsy and analysis of any skin lesion that has signs of being potentially cancerous."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Melanoma is the most dangerous type of skin cancer. Globally, in 2012, it newly occurred in 232,000 people. In 2015, 3.1 million people had active disease, which resulted in 59,800 deaths. Australia and New Zealand have the highest rates of melanoma in the world. High rates also occur in Northern Europe and North America, while it is less common in Asia, Africa, and Latin America. In the United States, melanoma occurs about 1.6 times more often in men than women. Melanoma has become more common since the 1960s in areas mostly populated by people of European descent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inception ResNet V2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Inception-ResNet-v2__ is a convolutional neural architecture that builds on the Inception family of architectures but incorporates residual connections (replacing the filter concatenation stage of the Inception architecture)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![CNN image](../assets/inception_resnet_v2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center; font-weight: bold\">Pic.2. Inception ResNetv 2 architecture</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to learn more about Inception ResNet v2, read [here](https://paperswithcode.com/method/inception-resnet-v2)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 638 images belonging to 2 classes.\n",
      "Found 159 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "generator = tf.keras.preprocessing.image.ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    rotation_range=180,\n",
    "    horizontal_flip=True,\n",
    "    vertical_flip=True,\n",
    "    brightness_range=(0.2, 1.5),\n",
    "    validation_split=0.2,\n",
    ")\n",
    "\n",
    "\n",
    "training_set = generator.flow_from_directory(\n",
    "    '/small-data', \n",
    "    target_size=(224, 224), \n",
    "    batch_size=32, \n",
    "    class_mode='categorical', \n",
    "    subset='training'\n",
    ")\n",
    "validation_set = generator.flow_from_directory(\n",
    "    '/small-data', \n",
    "    target_size=(224, 224), \n",
    "    batch_size=32, \n",
    "    class_mode='categorical', \n",
    "    subset='validation'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "CLASS_NUMBER = len(training_set.class_indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data source"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a data source, we use the ISIC Archive."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ISIC Archive is an open source platform with publicly available images of skin lesions under Creative Commons licenses. The images are associated with ground-truth diagnoses and other clinical metadata. Images can be queried using faceted search and downloaded individually or in batches. The initial focus of the archive has been on dermoscopy images of individual skin lesions, as these images are inherently standardized by the use of a specialized acquisition device and devoid of many of the privacy challenges associated with clinical images. To date, the images have been provided by specialized melanoma centers from around the world. The archive is designed to accept contributions from new sources under the Terms of Use and welcomes new contributors. There are ongoing efforts to supplement the dermoscopy images in the archive with close-up clinical images and a broader representation of skin types. The images in the Archive are used to support educational efforts through linkage with Dermoscopedia and are used for Grand Challenges and Live Challenges to engage the computer science community for the development of diagnostic AI."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For more information, go to [ISIC Archive web site](https://www.isic-archive.com/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We take the model from TensorFlow Hub. [Look here](https://tfhub.dev/google/tf2-preview/mobilenet_v2/feature_vector/4)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.Sequential([\n",
    "    hub.KerasLayer(\"https://tfhub.dev/google/imagenet/inception_resnet_v2/feature_vector/4\", output_shape=[1280],\n",
    "                   trainable=False),\n",
    "    tf.keras.layers.Dense(CLASS_NUMBER, activation='softmax')\n",
    "])\n",
    "model.build([None, 224, 224, 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " keras_layer (KerasLayer)    (None, 1536)              54336736  \n",
      "                                                                 \n",
      " dense (Dense)               (None, 2)                 3074      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 54,339,810\n",
      "Trainable params: 3,074\n",
      "Non-trainable params: 54,336,736\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing TensorFlow callbacks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our convenience, we create a few TensorFlow callbacks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The TensorBoard callback"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to see how the training is going. We add the callback, which will log the metrics to TensorBoard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dir = '../logs/fit/' + datetime.datetime.now().strftime('inceptionresnetv2')\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The EarlyStopping callback"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This callback stops training when the metrics (e.g. validation loss) are not improving,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stop_callback = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor=\"val_loss\", \n",
    "    min_delta=0.01, \n",
    "    patience=10, \n",
    "    restore_best_weights=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The ModelCheckpoint callback"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This callback saves the model with the best metrics during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path = 'checkpoints/inceptionresnetv2.ckpt'\n",
    "\n",
    "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    checkpoint_path,\n",
    "    monitor='val_loss',\n",
    "    verbose=1,\n",
    "    save_best_only=True,\n",
    "    save_weights_only=False,\n",
    "    save_freq='epoch',\n",
    "    mode='auto'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "20/20 [==============================] - ETA: 0s - loss: 0.3676 - accuracy: 0.8354\n",
      "Epoch 00001: val_loss improved from inf to 0.49989, saving model to checkpoints/inceptionresnetv2.ckpt\n",
      "INFO:tensorflow:Assets written to: checkpoints/inceptionresnetv2.ckpt/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: checkpoints/inceptionresnetv2.ckpt/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20/20 [==============================] - 64s 3s/step - loss: 0.3676 - accuracy: 0.8354 - val_loss: 0.4999 - val_accuracy: 0.8239\n",
      "Epoch 2/200\n",
      "20/20 [==============================] - ETA: 0s - loss: 0.2929 - accuracy: 0.8621\n",
      "Epoch 00002: val_loss did not improve from 0.49989\n",
      "20/20 [==============================] - 46s 2s/step - loss: 0.2929 - accuracy: 0.8621 - val_loss: 0.5815 - val_accuracy: 0.7358\n",
      "Epoch 3/200\n",
      "20/20 [==============================] - ETA: 0s - loss: 0.2670 - accuracy: 0.8793\n",
      "Epoch 00003: val_loss improved from 0.49989 to 0.49978, saving model to checkpoints/inceptionresnetv2.ckpt\n",
      "INFO:tensorflow:Assets written to: checkpoints/inceptionresnetv2.ckpt/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: checkpoints/inceptionresnetv2.ckpt/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20/20 [==============================] - 55s 3s/step - loss: 0.2670 - accuracy: 0.8793 - val_loss: 0.4998 - val_accuracy: 0.7736\n",
      "Epoch 4/200\n",
      "20/20 [==============================] - ETA: 0s - loss: 0.2549 - accuracy: 0.8856\n",
      "Epoch 00004: val_loss did not improve from 0.49978\n",
      "20/20 [==============================] - 46s 2s/step - loss: 0.2549 - accuracy: 0.8856 - val_loss: 0.5034 - val_accuracy: 0.7736\n",
      "Epoch 5/200\n",
      "20/20 [==============================] - ETA: 0s - loss: 0.2480 - accuracy: 0.8871\n",
      "Epoch 00005: val_loss did not improve from 0.49978\n",
      "20/20 [==============================] - 46s 2s/step - loss: 0.2480 - accuracy: 0.8871 - val_loss: 0.5664 - val_accuracy: 0.7233\n",
      "Epoch 6/200\n",
      "20/20 [==============================] - ETA: 0s - loss: 0.2409 - accuracy: 0.8934\n",
      "Epoch 00006: val_loss did not improve from 0.49978\n",
      "20/20 [==============================] - 46s 2s/step - loss: 0.2409 - accuracy: 0.8934 - val_loss: 0.5377 - val_accuracy: 0.8302\n",
      "Epoch 7/200\n",
      "20/20 [==============================] - ETA: 0s - loss: 0.2573 - accuracy: 0.8824"
     ]
    }
   ],
   "source": [
    "model.fit(\n",
    "    training_set, \n",
    "    validation_data=validation_set, \n",
    "    epochs=200, \n",
    "    callbacks=[\n",
    "                tensorboard_callback,\n",
    "                checkpoint_callback,\n",
    "                early_stop_callback\n",
    "            ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load the model with the best metrics (e.g. validation loss) from the checkpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.Sequential([\n",
    "    hub.KerasLayer(\"https://tfhub.dev/google/imagenet/inception_resnet_v2/feature_vector/4\", output_shape=[1280],\n",
    "                   trainable=False),\n",
    "    tf.keras.layers.Dense(CLASS_NUMBER, activation='softmax')\n",
    "])\n",
    "model.build([None, 224, 224, 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights('./checkpoints/inceptionresnetv2.ckpt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_set = generator.flow_from_directory(\n",
    "    '/small-data-test',\n",
    "    target_size=(224, 224),\n",
    "    batch_size=32,\n",
    "    class_mode='categorical'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making diagnoses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_labels = np.concatenate([testing_set[i][1] for i in range(len(testing_set))], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_labels = model.predict(testing_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the ROC Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr = dict()\n",
    "tpr = dict()\n",
    "auc_metric = dict()\n",
    "\n",
    "diagnosis_index_dict = {v: k for k, v in testing_set.class_indices.items()}\n",
    "\n",
    "for i in range(CLASS_NUMBER):\n",
    "    diagnosis = diagnosis_index_dict[i]\n",
    "    fpr[diagnosis], tpr[diagnosis], _ = roc_curve(true_labels[:, i], predicted_labels[:, i])\n",
    "    auc_metric[diagnosis] = auc(fpr[diagnosis], tpr[diagnosis])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for diagnosis in testing_set.class_indices:\n",
    "    plt.plot(fpr[diagnosis], tpr[diagnosis], label=diagnosis)\n",
    "    \n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver operating characteristic')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show AUC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auc_metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
