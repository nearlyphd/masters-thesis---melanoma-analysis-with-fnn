{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Analiza czerniaka za pomocą fraktalnej sieci neuronowej"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.ndimage import measurements\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow_addons as tfa\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sprawdzamy dostępne urządzenie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.config.list_physical_devices('GPU')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Zapisujemy konfigurację do zmiennych."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "IMAGE_SIZE = 224\n",
    "BATCH_SIZE = 32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tworzymy callbacki do zbierania danych o wydajności modelu do Tensorboard, zapisywania modelu w trakcie jego trenowania i zatrzymania trenowania modelu, jeśli nie ma poprawy w wynikach w ciągu 10 epok. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dir = '../logs/fit/' + datetime.datetime.now().strftime('fractal_net')\n",
    "tensorboard_callback = TensorBoard(log_dir=log_dir, histogram_freq=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path = 'checkpoints/fractal_net.ckpt'\n",
    "checkpoint_dir = os.path.dirname(checkpoint_path)\n",
    "\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    checkpoint_path,\n",
    "    monitor='val_loss',\n",
    "    verbose=1,\n",
    "    save_best_only=True,\n",
    "    save_weights_only=False,\n",
    "    save_freq='epoch',\n",
    "    mode='auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stop_callback = EarlyStopping(monitor=\"val_loss\", min_delta=0.01, patience=10, restore_best_weights=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Definiujemy warstwę, która będzie tworzyła obraz z fraktalnych cech podanych jej obrazów."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class Fractal2D(tf.keras.layers.Layer):\n",
    "    PERCOLATION_THRESHOLD = 0.59275\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(Fractal2D, self).__init__(name='fractal_layer')\n",
    "        self.kernel_size_range = (3, 41)\n",
    "\n",
    "    def chessboard_distance(self, patched_inputs, central_pixels, kernel_size):\n",
    "        return tf.cast(\n",
    "            tf.math.less_equal(\n",
    "                tf.math.reduce_max(\n",
    "                    tf.math.abs(tf.math.subtract(patched_inputs, central_pixels)), \n",
    "                    axis=3), \n",
    "                kernel_size), \n",
    "            dtype=tf.int32)\n",
    "    \n",
    "    def euclidean_distance(self, patched_inputs, central_pixels, kernel_size):\n",
    "        return tf.cast(\n",
    "            tf.math.less_equal(\n",
    "                tf.math.pow(\n",
    "                    tf.math.reduce_sum(\n",
    "                        tf.math.pow(\n",
    "                            tf.math.subtract(patched_inputs, central_pixels), \n",
    "                            2), \n",
    "                        axis=3), \n",
    "                    0.5), \n",
    "                kernel_size), \n",
    "            dtype=tf.int32)\n",
    "    \n",
    "    def manhattan_distance(self, patched_inputs, central_pixels, kernel_size):\n",
    "        return tf.cast(\n",
    "            tf.math.less_equal(\n",
    "                tf.math.reduce_sum(\n",
    "                    tf.math.abs(tf.math.subtract(patched_inputs, central_pixels)), \n",
    "                    axis=3), \n",
    "                kernel_size), \n",
    "            dtype=tf.int32)\n",
    "    \n",
    "    def extract_binary_patches(self, inputs, kernel_size, distance_function):\n",
    "        patched_inputs = tf.image.extract_patches(inputs,\n",
    "                                                     sizes=(1, kernel_size, kernel_size, 1),\n",
    "                                                     strides=(1, kernel_size, kernel_size, 1),\n",
    "                                                     rates=(1, 1, 1, 1),\n",
    "                                                     padding='SAME')\n",
    "        _, rows, cols, _ = patched_inputs.shape\n",
    "        patched_inputs = tf.reshape(patched_inputs, shape=(-1, kernel_size, kernel_size, 3))\n",
    "        \n",
    "        central_pixels = tf.image.resize_with_crop_or_pad(patched_inputs, 1, 1)\n",
    "        \n",
    "        return tf.reshape(distance_function(patched_inputs, central_pixels, kernel_size), \n",
    "                          shape=(-1, rows * cols, kernel_size, kernel_size))\n",
    "    \n",
    "    def calculate_probability_matrices(self, binary_inputs, kernel_size):\n",
    "        number_of_ones = tf.map_fn(lambda binary_input: tf.map_fn(lambda binary_patch: tf.math.reduce_sum(binary_patch), \n",
    "                                                                  binary_input), \n",
    "                                   binary_inputs)\n",
    "        _, patch_number = number_of_ones.shape\n",
    "        return tf.math.bincount(number_of_ones,\n",
    "                                minlength=1, \n",
    "                                maxlength=kernel_size ** 2 + 1, \n",
    "                                axis=-1) / patch_number\n",
    "    \n",
    "    def calculate_fractal_dimensions(self, probability_matrices):\n",
    "        def fd_helper(matrix):\n",
    "            return tf.math.reduce_sum(tf.math.divide(matrix, tf.range(1, len(matrix) + 1, dtype=tf.float64)))\n",
    "        return tf.map_fn(lambda matrix: fd_helper(matrix), probability_matrices)\n",
    "    \n",
    "    def calculate_lacunarity(self, probability_matrices):\n",
    "        def m_helper(matrix):\n",
    "            return tf.math.reduce_sum(tf.math.multiply(matrix, tf.range(1, len(matrix) + 1, dtype=tf.float64)))\n",
    "        \n",
    "        def m2_helper(matrix):\n",
    "            return tf.math.reduce_sum(tf.math.multiply(tf.math.pow(matrix, 2), tf.range(1, len(matrix) + 1, dtype=tf.float64)))\n",
    "        \n",
    "        return tf.map_fn(lambda probability_matrix: \n",
    "                         tf.math.divide(\n",
    "                             tf.math.subtract(m2_helper(probability_matrix), \n",
    "                                               tf.math.pow(m_helper(probability_matrix), 2)), \n",
    "                             tf.math.pow(m_helper(probability_matrix), 2)), \n",
    "                         probability_matrices)\n",
    "    \n",
    "    def average_cluster_percolation(self, binary_inputs, kernel_size):\n",
    "        number_of_ones = tf.map_fn(lambda binary_input: tf.map_fn(lambda binary_patch: tf.math.reduce_sum(binary_patch), \n",
    "                                                                  binary_input), \n",
    "                                   binary_inputs)\n",
    "        \n",
    "        return tf.math.reduce_mean(\n",
    "                        tf.cast(\n",
    "                            tf.math.greater_equal(\n",
    "                                tf.math.divide(number_of_ones, kernel_size ** 2), \n",
    "                                self.PERCOLATION_THRESHOLD), \n",
    "                            dtype=tf.int32), \n",
    "                    axis=1)\n",
    "    \n",
    "    def average_cluster_number(self, binary_inputs):\n",
    "        return tf.math.reduce_mean(\n",
    "            tf.map_fn(\n",
    "                lambda binary_input: tf.map_fn(\n",
    "                    lambda patch: tf.math.reduce_max(tfa.image.connected_components(patch)), \n",
    "                    binary_input), \n",
    "                binary_inputs), \n",
    "            axis=1)\n",
    "        \n",
    "    def average_cluster_max_area(self, binary_inputs):    \n",
    "        def most_common(array):\n",
    "            _, _, counts = tf.unique_with_counts(array)\n",
    "            return tf.math.reduce_max(counts)\n",
    "        \n",
    "        return tf.math.reduce_mean(\n",
    "                tf.map_fn(lambda binary_input: \n",
    "                            tf.map_fn(lambda patch: \n",
    "                                        most_common(tf.reshape(tfa.image.connected_components(patch), shape=(-1,))), \n",
    "                                      binary_input), \n",
    "                          binary_inputs), axis=1)\n",
    "\n",
    "    def calculate_components(self, inputs, kernel_size, distance_function):\n",
    "        binary_patches = self.extract_binary_patches(inputs, kernel_size, distance_function)\n",
    "\n",
    "        probability_matrices = self.calculate_probability_matrices(binary_patches, kernel_size)\n",
    "        fractal_dimensions = self.calculate_fractal_dimensions(probability_matrices)\n",
    "        lacunarity = self.calculate_lacunarity(probability_matrices)\n",
    "\n",
    "        average_cluster_percolation = self.average_cluster_percolation(binary_patches, kernel_size)\n",
    "        average_cluster_number = self.average_cluster_number(binary_patches)\n",
    "        average_cluster_max_area = self.average_cluster_max_area(binary_patches)\n",
    "\n",
    "        return tf.convert_to_tensor((average_cluster_number,\n",
    "                                    average_cluster_percolation,\n",
    "                                    average_cluster_max_area,\n",
    "                                    lacunarity,\n",
    "                                    fractal_dimensions), dtype=tf.float64)\n",
    "    \n",
    "    def rearrage_metrics(self, components):\n",
    "        def helper(components_input):\n",
    "            length, = components_input.shape\n",
    "            \n",
    "            rearranged_components = tf.concat([\n",
    "                tf.boolean_mask(components_input, tf.range(length) % 5 == 0),\n",
    "                tf.boolean_mask(components_input, tf.range(length) % 5 == 1),\n",
    "                tf.boolean_mask(components_input, tf.range(length) % 5 == 2),\n",
    "                tf.boolean_mask(components_input, tf.range(length) % 5 == 3),\n",
    "                tf.boolean_mask(components_input, tf.range(length) % 5 == 4),\n",
    "            ], axis=0)\n",
    "            return rearranged_components\n",
    "        return tf.map_fn(helper, components)\n",
    "    \n",
    "    \n",
    "    def call(self, inputs):\n",
    "        kernel_size_start, kernel_size_end = self.kernel_size_range\n",
    "\n",
    "        \n",
    "        cd_components, ed_components, md_components = [], [], []\n",
    "        for kernel_size in range(kernel_size_start, kernel_size_end + 1, 2):\n",
    "            cd_components.append(\n",
    "                tf.transpose(\n",
    "                    self.calculate_components(inputs,\n",
    "                                           kernel_size, \n",
    "                                           distance_function=self.chessboard_distance)))\n",
    "            ed_components.append(\n",
    "                tf.transpose(\n",
    "                    self.calculate_components(inputs,\n",
    "                                           kernel_size, \n",
    "                                           distance_function=self.euclidean_distance)))\n",
    "            md_components.append(\n",
    "                tf.transpose(\n",
    "                    self.calculate_components(inputs,\n",
    "                                           kernel_size, \n",
    "                                           distance_function=self.manhattan_distance)))\n",
    "            \n",
    "        cd_components = tf.reshape(self.rearrage_metrics(tf.concat(cd_components, axis=1)), shape=(-1, 10, 10))\n",
    "        ed_components = tf.reshape(self.rearrage_metrics(tf.concat(ed_components, axis=1)), shape=(-1, 10, 10))\n",
    "        md_components = tf.reshape(self.rearrage_metrics(tf.concat(md_components, axis=1)), shape=(-1, 10, 10))\n",
    "        \n",
    "        outputs = tf.concat([tf.expand_dims(cd_components, axis=3), \n",
    "                             tf.expand_dims(ed_components, axis=3),\n",
    "                             tf.expand_dims(md_components, axis=3)], \n",
    "                            axis=3)\n",
    "        \n",
    "        return tf.image.resize(outputs, size=(224, 224))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Ładujemy dane do trenowania i walidacji."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "datagen = ImageDataGenerator(validation_split=0.2, rescale=1.0)\n",
    "training_set = datagen.flow_from_directory('/small-data',\n",
    "                                           target_size=(IMAGE_SIZE, IMAGE_SIZE),\n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           class_mode='categorical',\n",
    "                                           subset='training')\n",
    "validation_set = datagen.flow_from_directory('/small-data',\n",
    "                                             target_size=(IMAGE_SIZE, IMAGE_SIZE),\n",
    "                                             batch_size=BATCH_SIZE,\n",
    "                                             class_mode='categorical',\n",
    "                                             subset='validation')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Zapisujemy ilość rozpoznawalnych diagnoz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "DIAGNOSIS_NUMBER = len(training_set.class_indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Tworzymy model, który wykorzystuje wcześniej zdefiniowaną warstwę."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fractal_model = tf.keras.Sequential([\n",
    "    tf.keras.layers.InputLayer(input_shape=(IMAGE_SIZE, IMAGE_SIZE, 3)),\n",
    "    Fractal2D(),\n",
    "    hub.KerasLayer(\"https://tfhub.dev/google/tf2-preview/mobilenet_v2/feature_vector/4\", \n",
    "                   output_shape=[1280],\n",
    "                   trainable=False),\n",
    "    tf.keras.layers.Dense(DIAGNOSIS_NUMBER, activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "fractal_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fractal_model.fit(training_set, validation_data=validation_set, epochs=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tworzymy model, który pracuje bezpośrednio z obrazkami."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_model = Sequential([\n",
    "    hub.KerasLayer(\"https://tfhub.dev/google/tf2-preview/mobilenet_v2/feature_vector/4\", output_shape=[1280],\n",
    "                   trainable=False),\n",
    "    Dense(DIAGNOSIS_NUMBER, activation='softmax')\n",
    "])\n",
    "original_model.build([None, 224, 224, 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_model.fit(training_set, validation_data=validation_set, epochs=2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
