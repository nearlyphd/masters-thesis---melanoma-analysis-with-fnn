{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b77dcd42-0127-4bc2-a683-d74e436e51f8",
   "metadata": {},
   "source": [
    "# Lesion classification with fractal neural networks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31059070-8be7-4b33-90b4-2ee8020b9326",
   "metadata": {},
   "source": [
    "__TL;DR__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea9df89b-8b53-48ef-882f-350769821519",
   "metadata": {},
   "source": [
    "We create a fractal neural network with an Inception ResNet V2 in its core and study its performance for lesion classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fabef0f2-f81d-47f1-b4f2-26ef6ea954bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from random import shuffle\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow_addons as tfa\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import Image, display\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4516cde0-d5d1-4187-8a48-decd0fc7b35c",
   "metadata": {},
   "source": [
    "Remove excessive logging. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "548c0142-3c08-4bd1-a007-faec9fa323cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.get_logger().setLevel('ERROR')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a38d0fe0-1492-427e-8252-8704b2b4d05b",
   "metadata": {},
   "source": [
    "# Melanoma"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0024b9a-9628-4058-8c7f-46ef06180c70",
   "metadata": {},
   "source": [
    "__Melanoma__, also redundantly known as __malignant melanoma__, is a type of skin cancer that develops from the pigment-producing cells known as melanocytes. Melanomas typically occur in the skin, but may rarely occur in the mouth, intestines, or eye (uveal melanoma). In women, they most commonly occur on the legs, while in men, they most commonly occur on the back. About 25% of melanomas develop from moles. Changes in a mole that can indicate melanoma include an increase in size, irregular edges, change in color, itchiness, or skin breakdown."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "196fcf25-834c-497b-b341-046ee62a62c7",
   "metadata": {},
   "source": [
    "![melanoma image](../assets/melanoma.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc2ba0f2-4663-4097-9faf-688b24d33643",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center; font-weight: bold\">Pic.1. A melanoma of approximately 2.5 cm (1 in) by 1.5 cm (0.6 in)</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abe8fd78-160e-40c7-bc7e-4f5a0339495a",
   "metadata": {},
   "source": [
    "The primary cause of melanoma is ultraviolet light (UV) exposure in those with low levels of the skin pigment melanin. The UV light may be from the sun or other sources, such as tanning devices. Those with many moles, a history of affected family members, and poor immune function are at greater risk. A number of rare genetic conditions, such as xeroderma pigmentosum, also increase the risk. Diagnosis is by biopsy and analysis of any skin lesion that has signs of being potentially cancerous."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0b4596a-4d63-400f-bd96-1429a80b0aff",
   "metadata": {},
   "source": [
    "Melanoma is the most dangerous type of skin cancer. Globally, in 2012, it newly occurred in 232,000 people. In 2015, 3.1 million people had active disease, which resulted in 59,800 deaths. Australia and New Zealand have the highest rates of melanoma in the world. High rates also occur in Northern Europe and North America, while it is less common in Asia, Africa, and Latin America. In the United States, melanoma occurs about 1.6 times more often in men than women. Melanoma has become more common since the 1960s in areas mostly populated by people of European descent."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f33be6e-0222-4ee2-b82c-2b7c65e90ad7",
   "metadata": {},
   "source": [
    "# Fractal neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "551d6dc0-4a6f-4a37-a3a5-4ebd71de694d",
   "metadata": {},
   "source": [
    "We propose an ensemble model based on handcrafted fractal features and deep learning that consists of combining the classification of two CNNs by applying the sum rule. We apply feature extraction to obtain 300 fractal features from different\n",
    "dermoscopy datasets. These features are reshaped into a 10 Ã— 10 Ã— 3 matrix to compose an artificial image that\n",
    "is given as input to the first CNN. The second CNN model receives as input the correspondent original image."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93d16b15-830c-4d82-9418-ba816524bb89",
   "metadata": {},
   "source": [
    "![CNN image](../assets/fnn.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91886815-2f81-4b09-a30c-57f83fb03baa",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center; font-weight: bold\">Pic.2. Overview of the proposed FNN model.</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4dceebf-b790-4206-b4ab-73d6b1e732b9",
   "metadata": {},
   "source": [
    "If you want to learn more about fractal neural networks, read [here](https://www.sciencedirect.com/science/article/abs/pii/S0957417420308563)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fedcb348-f370-4bf8-841f-dafc454916b1",
   "metadata": {},
   "source": [
    "## Dividing images into patches."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c4ea8cc-e462-4033-849b-96429720a7a2",
   "metadata": {},
   "source": [
    "According to the acticle:\n",
    "> One of the approaches available in the literature for multiscale analysis is the gliding-box algorithm (Ivanovici & Richard, 2011). The main advantage of this approach is that it can be applied on datasets containing images with different resolutions since the output features are given in relation to the scale instead of being absolute values. This algorithm consists in placing a box $\\beta_{i}$ sized $ð¿ Ã— ð¿$ on the left superior corner of the image, wherein ð¿ is given in pixels. This box glides through the image, one column and then one row at a time. After reaching the end of the image, the box is repositioned at the starting point and the value of ð¿ is increased by 2."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cb86671-e6b0-4efd-89ec-a7b58d326da4",
   "metadata": {},
   "source": [
    "The gliding-box method will not be used since it consumes too much RAM. We'll employ a box-counting approach, which basically means we'll partition the images into non-overlapping chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78d80020-d908-44cd-a6fd-328479da0afb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Patchify(tf.keras.layers.Layer):\n",
    "    def __init__(self, patch_size):\n",
    "        super(Patchify, self).__init__()\n",
    "        \n",
    "        self.patch_size = patch_size\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        outputs = tf.image.extract_patches(\n",
    "            inputs,\n",
    "            sizes=(1, self.patch_size, self.patch_size, 1),\n",
    "            strides=(1, self.patch_size, self.patch_size, 1),\n",
    "            rates=(1, 1, 1, 1),\n",
    "            padding='SAME'\n",
    "        )\n",
    "        \n",
    "        _, rows, cols, _ = tf.unstack(tf.shape(outputs))\n",
    "        outputs = tf.reshape(outputs, shape=(-1, rows * cols, self.patch_size, self.patch_size, 3))\n",
    "        \n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbb60628-947b-472a-9f72-ef9a3005d75f",
   "metadata": {},
   "source": [
    "## Creating an array of binary values from image patches using the Chebyshev colour distance function applied to the patch centre and each pixel."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdb75f97-5608-46d9-9b08-f0e5b81b78b3",
   "metadata": {},
   "source": [
    "According to the article:\n",
    "> For each time the box $\\beta_{i}$ is moved, a multidimensional analysis of colour similarity is performed for every pixel inside it. This is done by assigning the centre pixel to a vector $ð‘“_{c} = ð‘Ÿ_{c}, ð‘”_{c}, ð‘_{c}$, where $ð‘Ÿ_{c}, ð‘”_{c}$ and $ð‘_{c}$ correspond to the colour intensities for each of the RGB colour channels of given pixel. The other pixels in the box are assigned to a vector $ð‘“_{i} = ð‘Ÿ_{i}, ð‘”_{i}, ð‘_{i}$ and compared to the centre pixel by calculating a colour distance $\\Delta$. On the proposed approach, the Chebyshev ($\\Delta_{h}$) ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f640cf64-ca0b-4a43-acd2-daed1e9ded95",
   "metadata": {},
   "source": [
    "The following equation is used to compute the Chebyshev distance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2594a0b0-c3f4-459d-9a11-f80ea3f3c736",
   "metadata": {},
   "source": [
    "$$\n",
    "\\Delta_{h} = max(|f_{i}(k_{i}) - f_{c}(k_{c})|), k \\in r, g, b. \n",
    "$$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f9efcd3-a1c0-4d8b-aa7d-c17377bba7dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Chebyshev(tf.keras.layers.Layer):\n",
    "    def __init__(self):\n",
    "        super(Chebyshev, self).__init__()\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        batch_size, patch_number, patch_size, patch_size, channels = tf.unstack(tf.shape(inputs))\n",
    "        outputs = tf.reshape(inputs, shape=(-1, patch_number, patch_size, channels))    \n",
    "        \n",
    "        centers = tf.image.resize_with_crop_or_pad(outputs, 1, 1)\n",
    "\n",
    "        outputs = tf.math.subtract(outputs, centers)\n",
    "        outputs = tf.math.abs(outputs)\n",
    "        outputs = tf.math.reduce_max(outputs, axis=3)\n",
    "        outputs = tf.math.less_equal(outputs, tf.cast(patch_size, dtype=tf.float32))\n",
    "        outputs = tf.cast(outputs, dtype=tf.int32)\n",
    "        outputs = tf.reshape(outputs, shape=(-1, patch_number, patch_size, patch_size))\n",
    "        \n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ab0eae6-b60b-4857-8a40-7b9fdf1bf50b",
   "metadata": {},
   "source": [
    "## Creating an array of binary values from image patches using the Euclidean colour distance function applied to the patch centre and each pixel."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbd6f097-4d18-4489-b9c0-ccd5af2f5f2d",
   "metadata": {},
   "source": [
    "According to the article:\n",
    "> For each time the box $\\beta_{i}$ is moved, a multidimensional analysis of colour similarity is performed for every pixel inside it. This is done by assigning the centre pixel to a vector $ð‘“_{c} = ð‘Ÿ_{c}, ð‘”_{c}, ð‘_{c}$, where $ð‘Ÿ_{c}, ð‘”_{c}$ and $ð‘_{c}$ correspond to the colour intensities for each of the RGB colour channels of given pixel. The other pixels in the box are assigned to a vector $ð‘“_{i} = ð‘Ÿ_{i}, ð‘”_{i}, ð‘_{i}$ and compared to the centre pixel by calculating a colour distance $\\Delta$. On the proposed approach, ... the Euclidean ($\\Delta_{e}$) ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "901070e7-50c6-4b66-979b-fb2b608a7b6a",
   "metadata": {},
   "source": [
    "$$\n",
    "\\Delta_{e} = \\sqrt{\\sum_{k} (f_{i}(k_{i}) - f_{c}(k_{c}))^2}, k \\in r, g, b\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8e6c66d-7c3e-4c09-848f-780c299fdd5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Euclidean(tf.keras.layers.Layer):\n",
    "    def __init__(self):\n",
    "        super(Euclidean, self).__init__()\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        batch_size, patch_number, patch_size, patch_size, channels = tf.unstack(tf.shape(inputs))\n",
    "        outputs = tf.reshape(inputs, shape=(-1, patch_number, patch_size, channels))\n",
    "        \n",
    "        centers = tf.image.resize_with_crop_or_pad(outputs, 1, 1)\n",
    "\n",
    "        outputs = tf.math.subtract(outputs, centers)\n",
    "        outputs = tf.math.pow(outputs, 2)\n",
    "        outputs = tf.math.reduce_sum(outputs, axis=3)\n",
    "        outputs = tf.math.pow(outputs, 0.5)\n",
    "        outputs = tf.math.less_equal(outputs, tf.cast(patch_size, dtype=tf.float32))\n",
    "        outputs = tf.cast(outputs, dtype=tf.int32)\n",
    "        outputs = tf.reshape(outputs, shape=(-1, patch_number, patch_size, patch_size))\n",
    "        \n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8222a34-869b-45a6-8919-02b86885695d",
   "metadata": {},
   "source": [
    "## Creating an array of binary values from image patches using the Manhattan colour distance function applied to the patch centre and each pixel."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bf3c888-40e9-4651-a404-a6f70e945618",
   "metadata": {},
   "source": [
    "According to the article:\n",
    "> For each time the box $\\beta_{i}$ is moved, a multidimensional analysis of colour similarity is performed for every pixel inside it. This is done by assigning the centre pixel to a vector $ð‘“_{c} = ð‘Ÿ_{c}, ð‘”_{c}, ð‘_{c}$, where $ð‘Ÿ_{c}, ð‘”_{c}$ and $ð‘_{c}$ correspond to the colour intensities for each of the RGB colour channels of given pixel. The other pixels in the box are assigned to a vector $ð‘“_{i} = ð‘Ÿ_{i}, ð‘”_{i}, ð‘_{i}$ and compared to the centre pixel by calculating a colour distance $\\Delta$. On the proposed approach, ... the Manhattan ($\\Delta_{m}$) ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96b2b2b6-21eb-4fa4-b1ab-6e9fdd6fe184",
   "metadata": {},
   "source": [
    "$$\n",
    "\\Delta_{m} = \\sum_{k} |f_{i}(k_{i}) - f_{c}(k_{c})|, k \\in r, g, b\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5f11e0a-4a9a-41bc-96b0-427e5caa80f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Manhattan(tf.keras.layers.Layer):\n",
    "    def __init__(self):\n",
    "        super(Manhattan, self).__init__()\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        batch_size, patch_number, patch_size, patch_size, channels = tf.unstack(tf.shape(inputs))\n",
    "        outputs = tf.reshape(inputs, shape=(-1, patch_number, patch_size, channels))\n",
    "        \n",
    "        centers = tf.image.resize_with_crop_or_pad(outputs, 1, 1)\n",
    "\n",
    "        outputs = tf.math.subtract(outputs, centers)\n",
    "        outputs = tf.math.abs(outputs)\n",
    "        outputs = tf.math.reduce_sum(outputs, axis=3)\n",
    "        outputs = tf.math.less_equal(outputs, tf.cast(patch_size, dtype=tf.float32))\n",
    "        outputs = tf.cast(outputs, dtype=tf.int32)\n",
    "        outputs = tf.reshape(outputs, shape=(-1, patch_number, patch_size, patch_size))\n",
    "            \n",
    "        \n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "739b294a-c09b-4c97-ba76-e0fcae052f69",
   "metadata": {},
   "source": [
    "## Calculating probability matrices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8dc9014-d4f1-41f4-831c-e8e175fcf864",
   "metadata": {},
   "source": [
    "According to the article:\n",
    "> After performing this conversion for every box of every given ð¿ scale, a structure known as probability matrix is generated. Each element of the matrix corresponds to the probability ð‘ƒ that ð‘š pixels on a scale ð¿ are labelled as 1 on each box. ... The matrix is normalized in a way that the sum of the elements in a column is equal to 1, as showed here:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f545ff06-b3cb-4c4f-812b-3625dfd6e4c1",
   "metadata": {},
   "source": [
    "$$\n",
    "\\sum_{m=1}^{L^2} P(m, L) = 1, \\forall L\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9341b029-dea8-40c6-b470-229980bfcc8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Probability(tf.keras.layers.Layer):\n",
    "    def __init__(self):\n",
    "        super(Probability, self).__init__()\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        batch_size, patch_number, patch_size, patch_size = tf.unstack(tf.shape(inputs))\n",
    "        \n",
    "        outputs = tf.math.reduce_sum(inputs, axis=(2, 3))\n",
    "        outputs = tf.vectorized_map(lambda image: tf.math.bincount(image, minlength=patch_size ** 2 + 1), outputs)\n",
    "        outputs = tf.math.divide(outputs, patch_number)        \n",
    "        \n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01b3f3c7-4bb9-40eb-9eba-87b5cb895f54",
   "metadata": {},
   "source": [
    "## Calculating fractal dimensions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13ce86ac-f28a-462f-aed3-9c09fe1425f1",
   "metadata": {},
   "source": [
    "According to the article:\n",
    "> FD is the most common technique to evaluate the fractal properties of an image. This is a measure for evaluating the irregularity and the complexity of a fractal. To obtain local FD features from the probability\n",
    "matrix, for each value of ð¿, the FD denominated ð·(ð¿) is calculated according to"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab666ef5-6d9b-4110-93cf-f1cbfbd8671f",
   "metadata": {},
   "source": [
    "$$\n",
    "D(L) = \\sum_{m=1}^{L^2} \\frac{P(m, L)}{m}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bee53338-685c-4a0f-bc46-149d02ffaae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FractalDimension(tf.keras.layers.Layer):\n",
    "    def __init__(self):\n",
    "        super(FractalDimension, self).__init__()\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        batch_size, _len = tf.unstack(tf.shape(inputs))\n",
    "        numbers = tf.reshape(\n",
    "            tf.concat(\n",
    "                [tf.constant([1], dtype=tf.float32), tf.range(1, _len, dtype=tf.float32)], \n",
    "                axis=0\n",
    "            ), \n",
    "            shape=(1, -1)\n",
    "        )\n",
    "        \n",
    "        outputs = tf.math.divide(inputs, numbers)\n",
    "        outputs = tf.math.reduce_sum(outputs, axis=1)\n",
    "        \n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ca69647-3cb8-42b8-9050-57d46136bfc1",
   "metadata": {},
   "source": [
    "## Calculating lacunarity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfba9bfc-8c48-48d2-9d7d-01ad2efd591c",
   "metadata": {},
   "source": [
    "According to the article:\n",
    "> LAC is a measure complementary to FD and allows to evaluate how the space of a fractal is filled (Ivanovici & Richard, 2009). From the probability matrix, first and second-order moments are calculated with"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68234b22-1b04-4e43-bb41-3a645eb381aa",
   "metadata": {},
   "source": [
    "$$\n",
    "\\mu(L) = \\sum_{m=1}^{L^2} mP(m, L)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e5e0b02-ef2e-4e34-a1de-3eeb6f565ee6",
   "metadata": {},
   "source": [
    "$$\n",
    "\\mu^2(L) = \\sum_{m=1}^{L^2} m^{2}P(m, L)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c375d3d0-4c67-4f8f-b931-058dc697f6f2",
   "metadata": {},
   "source": [
    "> The LAC value for a scale $L$ is given by $\\Lambda$(ð¿), which is obtained according to"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "958afcea-efa1-4750-a877-48a96884748d",
   "metadata": {},
   "source": [
    "$$\n",
    "\\Lambda(L) = \\frac{\\mu^{2}(L) - (\\mu(L))^{2}}{(\\mu(L))^{2}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78cf6813-00c3-4bac-8c8e-ab7dc01a710c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Lacunarity(tf.keras.layers.Layer):\n",
    "    def __init__(self):\n",
    "        super(Lacunarity, self).__init__()\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        batch_size, _len = tf.unstack(tf.shape(inputs))\n",
    "        numbers = tf.reshape(\n",
    "            tf.concat(\n",
    "                [tf.constant([1], dtype=tf.float32), tf.range(1, _len, dtype=tf.float32)], \n",
    "                axis=0\n",
    "            ), \n",
    "            shape=(1, -1)\n",
    "        )\n",
    "                \n",
    "        mu_first_2 = tf.math.multiply(inputs, numbers)\n",
    "        mu_first_2 = tf.math.reduce_sum(mu_first_2, axis=1)\n",
    "        mu_first_2 = tf.math.pow(mu_first_2, 2)\n",
    "\n",
    "        mu_second = tf.math.pow(numbers, 2)\n",
    "        mu_second = tf.math.multiply(inputs, mu_second)\n",
    "        mu_second = tf.math.reduce_sum(mu_second, axis=1)\n",
    "\n",
    "        outputs = tf.math.divide(\n",
    "            tf.math.subtract(mu_second, mu_first_2),\n",
    "            mu_first_2\n",
    "        )\n",
    "        \n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6762fc0a-4e94-4ea0-b490-1c1edf19c8ab",
   "metadata": {},
   "source": [
    "## Calculating percolation Q - the average occurrence of percolation on a scale L"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfc9d2f8-1bcb-4125-b1df-01cebd34f251",
   "metadata": {},
   "source": [
    "According to the article:\n",
    "> We can also verify whether a box $\\beta_{i}$ is percolating. This can be achieved due to a property that states a percolation threshold for different types of structures. In squared matrices (digital images), this threshold has the value of $p = 0.59275$, which means that if the ratio between pixels labelled as 1 and pixels labelled as 0 is greater or equal than $p$, the matrix is considered as percolating. Let $\\Omega_{i}$ be the number of pixels labelled as 1 in a box $\\beta_{i}$ with size $L \\times L $ , we determine whether such box is percolating according to"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a1d071f-db8e-4cbd-bc8d-647386a221a2",
   "metadata": {},
   "source": [
    "$$\n",
    "q_{i} = \n",
    "\\begin{cases}\n",
    "1, & \\frac{\\Omega_{i}}{L^2} \\ge 0.59275 \\\\\n",
    "0, & \\frac{\\Omega_{i}}{L^2} < 0.59275\n",
    "\\end{cases}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7331f25-73de-4c10-894c-681d97065af2",
   "metadata": {},
   "source": [
    "> This results in a binary value for $q_{i}$, wherein 1 indicates that thebox is percolating. The feature $Q(L)$ regards the average occurrence of percolation on a scale $L$ and can be obtained by"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b8cdb94-a5fa-4853-bb36-7c5952cd416c",
   "metadata": {},
   "source": [
    "$$\n",
    "Q(L) = \\frac{\\sum_{i=1}^{T(L)} q_{i}}{T(L)}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b23e9db-96a5-4a1c-8186-166171793b76",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PercolationQ(tf.keras.layers.Layer):\n",
    "    def __init__(self, threshold=0.59275):\n",
    "        super(PercolationQ, self).__init__()\n",
    "        \n",
    "        self.threshold = threshold\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        batch_size, patch_number, patch_size, patch_size = tf.unstack(tf.shape(inputs))\n",
    "        \n",
    "        outputs = tf.math.reduce_sum(inputs, axis=(2, 3))\n",
    "        outputs = tf.math.divide(outputs, patch_size ** 2)\n",
    "        outputs = tf.math.greater_equal(outputs, self.threshold)\n",
    "        outputs = tf.cast(outputs, dtype=tf.float32)\n",
    "        outputs = tf.math.reduce_mean(outputs, axis=1)\n",
    "        \n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4160f1d-6009-48e3-a186-068ee8e141a4",
   "metadata": {},
   "source": [
    "## Clustering values in binarized patches "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b0fd48f-9857-40ca-8262-192e3f332a1f",
   "metadata": {},
   "source": [
    "The next two layers, which calculate percolation C and M, work with value clusters. We clustorize values in a separate layer to speed up calculations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dbbd0bd-119f-41c2-be84-41bd239aeda0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Clusterize(tf.keras.layers.Layer):\n",
    "    def __init__(self):\n",
    "        super(Clusterize, self).__init__()\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        batch_size, patch_number, patch_size, patch_size = tf.unstack(tf.shape(inputs))\n",
    "        \n",
    "        outputs = tf.reshape(inputs, shape=(-1, patch_size, patch_size))\n",
    "        outputs = tfa.image.connected_components(outputs)\n",
    "        outputs = tf.reshape(outputs, shape=(-1, patch_number, patch_size, patch_size))\n",
    "        \n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "190c5b81-d152-479e-97b2-3cf06fc28fe0",
   "metadata": {},
   "source": [
    "## Calculating percolation C - the average number of clusters per box on a scale L"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74ae9f11-a288-4f0c-b290-c8096a621280",
   "metadata": {},
   "source": [
    "According to the article:\n",
    "> Let $c_{i}$ be the number of clusters on a box $\\beta_{i}$, the feature $C(L)$ that represents the average number of clusters per box on a scale $L$ is given by"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c9e3be5-2155-4f51-a9bd-32879164b5c1",
   "metadata": {},
   "source": [
    "$$\n",
    "C(L) = \\frac{\\sum_{i=1}^{T(L)} c_{i}}{T(L)}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39ab84d3-af0b-4e65-af6d-8277e5cd5257",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PercolationC(tf.keras.layers.Layer):\n",
    "    def __init__(self):\n",
    "        super(PercolationC, self).__init__()\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        outputs = tf.cast(inputs, dtype=tf.float32)\n",
    "        outputs = tf.math.reduce_max(outputs, axis=(2, 3))\n",
    "        outputs = tf.math.reduce_mean(outputs, axis=1)\n",
    "        \n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e730b17-1e79-40b6-ae47-cd2d8c87511c",
   "metadata": {},
   "source": [
    "## Calculating percolation M - the average coverage area of the largest cluster on a scale L"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98c61475-4707-424c-a52e-babcc135660b",
   "metadata": {},
   "source": [
    "According to the article:\n",
    ">Another feature that can be obtained is the average coverage area of the largest cluster in a box and is given by $M(L)$. Let $m_{i}$ be the size in pixels of the largest cluster of the box $\\beta_{i}$. The feature $M(L)$ is givenaccording to"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ee7738f-7a48-4e31-b3c2-53f47fd8fc91",
   "metadata": {},
   "source": [
    "$$\n",
    "M(L) = \\frac{\\sum_{i=1}^{T(L)} \\frac{m_{i}}{L^2}}{T(L)}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a892113-3a84-415f-a58e-89e32f8536a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PercolationM(tf.keras.layers.Layer):\n",
    "    def __init__(self):\n",
    "        super(PercolationM, self).__init__()\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        batch_size, patch_number, patch_size, patch_size = tf.unstack(tf.shape(inputs))\n",
    "        \n",
    "        outputs = tf.reshape(inputs, shape=(-1, patch_number, patch_size ** 2))\n",
    "        outputs = tf.map_fn(lambda image: tf.math.reduce_max(tf.math.bincount(image)), outputs)\n",
    "        outputs = tf.cast(outputs, dtype=tf.float32)\n",
    "        \n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b5e1944-7b85-4a72-9d18-1f8e11f5b440",
   "metadata": {},
   "source": [
    "## Assembling fractal features into an image channel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dff97030-41d1-4c7d-b5b4-004885a8161c",
   "metadata": {},
   "source": [
    "According to the article:\n",
    "> To serve as input for the incoming CNN classification, the feature vectors generated on the previous layers of the network must be converted into feature matrices. To do so, the 100 features obtained by each distance $\\Delta$ are rearranged as a $10 \\times 10 \\times 10$ matrix. The matrices generated by $\\Delta_{h}$, $\\Delta_{e}$ and $\\Delta_{m}$ correspond to the R, G and B colour channels, respectively. ... Since each of the functions $C(L), Q(L), M(L), \\Lambda(L)$ and $D(L)$, obtained from a specific $\\Delta$, generate 20 features, each function is fit exactly into 2 columns of the matrix.\n",
    "\n",
    ">Since each of the functions $C(L), Q(L), M(L), \\Lambda(L)$ and $D(L)$, obtained from a specific $\\Delta$, generate 20 features, each function is fit exactly into 2 columns of the matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c950512d-6df3-47df-947a-2efaf53b5220",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AssembleChannel(tf.keras.layers.Layer):\n",
    "    def __init__(self):\n",
    "        super(AssembleChannel, self).__init__()\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        fractal_dimension = tf.convert_to_tensor(inputs[0])\n",
    "        fractal_dimension = tf.transpose(fractal_dimension, perm=(1, 0))\n",
    "        \n",
    "        lacunarity = tf.convert_to_tensor(inputs[1])\n",
    "        lacunarity = tf.transpose(lacunarity, perm=(1, 0))\n",
    "        \n",
    "        percolation_q = tf.convert_to_tensor(inputs[2])\n",
    "        percolation_q = tf.transpose(percolation_q, perm=(1, 0))\n",
    "        \n",
    "        percolation_c = tf.convert_to_tensor(inputs[3])\n",
    "        percolation_c = tf.transpose(percolation_c, perm=(1, 0))\n",
    "        \n",
    "        percolation_m = tf.convert_to_tensor(inputs[4])\n",
    "        percolation_m = tf.transpose(percolation_m, perm=(1, 0))\n",
    "        \n",
    "        outputs = tf.concat([\n",
    "            percolation_c,\n",
    "            percolation_q,\n",
    "            percolation_m,\n",
    "            lacunarity,\n",
    "            fractal_dimension\n",
    "        ], axis=1)\n",
    "        outputs = tf.reshape(outputs, shape=(-1, 10, 10))\n",
    "        \n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7c32426-a3dd-4fdd-8b50-a7719360d759",
   "metadata": {},
   "source": [
    "## Organising fractal feature extraction into layers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2d73f38-64ca-4e0f-8180-2556e494dc8e",
   "metadata": {},
   "source": [
    "We move feature extraction to layers to simplify and clarify the code."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e03a3e40-3648-4ce9-aea4-f4695c193fe5",
   "metadata": {},
   "source": [
    "### based on Chebyshev distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0253836-4792-4ae3-ab29-4a0943ee11f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChebyshevFeatures(tf.keras.layers.Layer):\n",
    "    def __init__(self):\n",
    "        super(ChebyshevFeatures, self).__init__()\n",
    "        \n",
    "        self.chebyshev = Chebyshev()\n",
    "        self.probability = Probability()\n",
    "        self.clusterize = Clusterize()\n",
    "        \n",
    "        self.fractal_dimension = FractalDimension()\n",
    "        self.lacunarity = Lacunarity()\n",
    "        self.percolation_q = PercolationQ()\n",
    "        self.percolation_c = PercolationC()\n",
    "        self.percolation_m = PercolationM()\n",
    "        \n",
    "        self.assemble_channel = AssembleChannel()\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        chebyshevs = [self.chebyshev(i) for i in inputs]\n",
    "        \n",
    "        probability = [self.probability(ch) for ch in chebyshevs]\n",
    "        cluster = [self.clusterize(ch) for ch in chebyshevs]\n",
    "        \n",
    "        fractal_dimension = [self.fractal_dimension(ch) for ch in probability]\n",
    "        lacunarity = [self.lacunarity(ch) for ch in probability]        \n",
    "        percolation_q = [self.percolation_q(ch) for ch in chebyshevs]\n",
    "        percolation_c = [self.percolation_c(ch) for ch in cluster]\n",
    "        percolation_m = [self.percolation_m(ch) for ch in cluster]\n",
    "        \n",
    "        features = self.assemble_channel([\n",
    "            fractal_dimension,\n",
    "            lacunarity,\n",
    "            percolation_q,\n",
    "            percolation_c,\n",
    "            percolation_m\n",
    "        ])\n",
    "        \n",
    "        return features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f080bbc9-ae81-4e04-afa5-02a7901b4665",
   "metadata": {},
   "source": [
    "### based on Euclidean distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23a81152-0b5e-46e4-9587-d35447187b09",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EuclideanFeatures(tf.keras.layers.Layer):\n",
    "    def __init__(self):\n",
    "        super(EuclideanFeatures, self).__init__()\n",
    "        \n",
    "        self.euclidean = Euclidean()\n",
    "        self.probability = Probability()\n",
    "        self.clusterize = Clusterize()\n",
    "        \n",
    "        self.fractal_dimension = FractalDimension()\n",
    "        self.lacunarity = Lacunarity()\n",
    "        self.percolation_q = PercolationQ()\n",
    "        self.percolation_c = PercolationC()\n",
    "        self.percolation_m = PercolationM()\n",
    "        \n",
    "        self.assemble_channel = AssembleChannel()\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        euclideans = [self.euclidean(i) for i in inputs]\n",
    "        \n",
    "        probability = [self.probability(eu) for eu in euclideans]\n",
    "        cluster = [self.clusterize(eu) for eu in euclideans]\n",
    "        \n",
    "        fractal_dimension = [self.fractal_dimension(eu) for eu in probability]\n",
    "        lacunarity = [self.lacunarity(eu) for eu in probability]        \n",
    "        percolation_q = [self.percolation_q(eu) for eu in euclideans]\n",
    "        percolation_c = [self.percolation_c(eu) for eu in cluster]\n",
    "        percolation_m = [self.percolation_m(eu) for eu in cluster]\n",
    "        \n",
    "        features = self.assemble_channel([\n",
    "            fractal_dimension,\n",
    "            lacunarity,\n",
    "            percolation_q,\n",
    "            percolation_c,\n",
    "            percolation_m\n",
    "        ])\n",
    "        \n",
    "        return features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb0c2efa-b386-40fd-8e80-7ea97b42657f",
   "metadata": {},
   "source": [
    "### based on Manhattan distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "392901a6-2b18-4f3a-8097-b597c1866e4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ManhattanFeatures(tf.keras.layers.Layer):\n",
    "    def __init__(self):\n",
    "        super(ManhattanFeatures, self).__init__()\n",
    "        \n",
    "        self.manhattan = Manhattan()\n",
    "        self.probability = Probability()\n",
    "        self.clusterize = Clusterize()\n",
    "        \n",
    "        self.fractal_dimension = FractalDimension()\n",
    "        self.lacunarity = Lacunarity()\n",
    "        self.percolation_q = PercolationQ()\n",
    "        self.percolation_c = PercolationC()\n",
    "        self.percolation_m = PercolationM()\n",
    "        \n",
    "        self.assemble_channel = AssembleChannel()\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        manhattans = [self.manhattan(i) for i in inputs]\n",
    "        \n",
    "        probability = [self.probability(mh) for mh in manhattans]\n",
    "        cluster = [self.clusterize(mh) for mh in manhattans]\n",
    "        \n",
    "        fractal_dimension = [self.fractal_dimension(mh) for mh in probability]\n",
    "        lacunarity = [self.lacunarity(mh) for mh in probability]        \n",
    "        percolation_q = [self.percolation_q(mh) for mh in manhattans]\n",
    "        percolation_c = [self.percolation_c(mh) for mh in cluster]\n",
    "        percolation_m = [self.percolation_m(mh) for mh in cluster]\n",
    "        \n",
    "        features = self.assemble_channel([\n",
    "            fractal_dimension,\n",
    "            lacunarity,\n",
    "            percolation_q,\n",
    "            percolation_c,\n",
    "            percolation_m\n",
    "        ])\n",
    "        \n",
    "        return features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1371f0e-1f5d-4802-929b-15765cce90b9",
   "metadata": {},
   "source": [
    "## Assembling fractal features into images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "231200fb-9cfd-4be0-8516-bf8014033ce0",
   "metadata": {},
   "source": [
    "We assemble fractal features into images, such that each set of fractal features corresponds to a colour channel (R, G, B)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0487d07b-9240-443b-b239-90afd43b2c0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AssembleImage(tf.keras.layers.Layer):\n",
    "    def __init__(self):\n",
    "        super(AssembleImage, self).__init__()\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        outputs = tf.stack(inputs)\n",
    "        outputs = tf.transpose(outputs, perm=(1, 2, 3, 0))\n",
    "        \n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b2eb911-48bd-45b0-9a9f-dbcee142ce41",
   "metadata": {},
   "source": [
    "## Organising the fractal feature extraction layers into the single, fractal image layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c927d03a-f5ba-4ce8-adba-85aa8a97a54e",
   "metadata": {},
   "source": [
    "To further simplify the code, we will gather the fractal feature extraction into the single layer, which generates artificial fractal image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65f4c2cb-caa0-4098-bab3-5fa030fb6d7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FractalImage(tf.keras.layers.Layer):\n",
    "    def __init__(self):\n",
    "        super(FractalImage, self).__init__()\n",
    "        \n",
    "        self.patchifies = [Patchify(patch_size) for patch_size in range(3, 41 + 1, 2)]\n",
    "        \n",
    "        self.chebyshev_features = ChebyshevFeatures()\n",
    "        self.euclidean_features = EuclideanFeatures()\n",
    "        self.manhattan_features = ManhattanFeatures()\n",
    "        \n",
    "        self.assemble_image = AssembleImage()\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        patchifies = [patchify(inputs) for patchify in self.patchifies]\n",
    "        \n",
    "        chebyshev_features = self.chebyshev_features(patchifies)\n",
    "        euclidean_features = self.euclidean_features(patchifies)\n",
    "        manhattan_features = self.manhattan_features(patchifies)\n",
    "        \n",
    "        outputs = self.assemble_image([\n",
    "            chebyshev_features,\n",
    "            euclidean_features,\n",
    "            manhattan_features\n",
    "        ])\n",
    "        \n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5091db2-0c37-42de-a474-584af5cec9ba",
   "metadata": {},
   "source": [
    "# Assembling the fractal neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89d48656-0069-4ddd-ba53-062de69277a4",
   "metadata": {},
   "source": [
    "So, here we are assembling the fractal neural network from the pieces mentioned above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31d1effb-b9c2-4863-943b-85c49f330994",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FractalNeuralNetwork(tf.keras.Model):\n",
    "    TARGET_WIDTH, TARGET_HEIGHT = 299, 299\n",
    "    \n",
    "    def __init__(self, class_number):\n",
    "        super(FractalNeuralNetwork, self).__init__()\n",
    "        \n",
    "        self.fractal_image = FractalImage()\n",
    "        self.resize = tf.keras.layers.Resizing(\n",
    "            width=self.TARGET_WIDTH,\n",
    "            height=self.TARGET_HEIGHT\n",
    "        )\n",
    "        self.rescale = tf.keras.layers.Rescaling(scale=1./255)\n",
    "        \n",
    "        self.original_model = tf.keras.applications.inception_resnet_v2.InceptionResNetV2(\n",
    "            include_top=False,\n",
    "            weights='imagenet',\n",
    "            input_shape=(299, 299, 3),\n",
    "            pooling='avg'\n",
    "        )\n",
    "        for layer in self.original_model.layers:\n",
    "            if layer.name in ['conv_7b', 'block8_10_conv']:\n",
    "                layer.trainable = True\n",
    "            else:\n",
    "                layer.trainable = False\n",
    "                \n",
    "        self.fractal_model = tf.keras.applications.inception_resnet_v2.InceptionResNetV2(\n",
    "            include_top=False,\n",
    "            weights='imagenet',\n",
    "            input_shape=(299, 299, 3),\n",
    "            pooling='avg'\n",
    "        )\n",
    "        for layer in self.fractal_model.layers:\n",
    "            if layer.name in ['conv_7b', 'block8_10_conv']:\n",
    "                layer.trainable = True\n",
    "            else:\n",
    "                layer.trainable = False\n",
    "                \n",
    "        self.combine = tf.keras.layers.Concatenate()\n",
    "        self.score = tf.keras.layers.Dense(class_number, activation='softmax')\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        fractal_outputs = self.fractal_image(inputs)\n",
    "        fractal_outputs = self.resize(fractal_outputs)\n",
    "        fractal_outputs = self.rescale(fractal_outputs)\n",
    "        fractal_outputs = self.fractal_model(fractal_outputs)\n",
    "        \n",
    "        original_outputs = self.rescale(inputs)\n",
    "        original_outputs = self.original_model(original_outputs)\n",
    "        \n",
    "        outputs = self.combine([fractal_outputs, original_outputs])\n",
    "        outputs = self.score(outputs)\n",
    "        \n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48a18a90-7030-4ac8-968e-808a869989bc",
   "metadata": {},
   "source": [
    "# Data research"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4b9df40-167f-49dd-ab1e-9dcd1a8db749",
   "metadata": {},
   "source": [
    "We load the __metadata.csv__, which contains information about each image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb495e34-1583-42a4-be12-322c268a78cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata = pd.read_csv(f\"{os.environ['SCRATCH']}/isic-archive/metadata.csv\")\n",
    "metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8625ad5-d80e-4420-9661-11c841a3cbd5",
   "metadata": {},
   "source": [
    "__Note__:\n",
    "As we discovered, there are invalid image files. Let's remove those from the metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6008bdd6-08d5-4e0e-8246-7fc2c98f02e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata = metadata.drop(metadata[metadata['isic_id'].isin(['ISIC_0060052', 'ISIC_0029842'])].index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35c7437b-6873-42a7-943f-07cec9a1a5b7",
   "metadata": {},
   "source": [
    "Two columns are particularly important for research: __benign_malignant__ - indicates whether the lesion is benign or malignant without any particular diagnosis, and __diagnosis__ column. \n",
    "\n",
    "We break down the data by __benign_malignant__ and __diagnosis__ to see, what is the balance there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "759c2307-2aa8-454a-8ab8-91c964ed7523",
   "metadata": {},
   "outputs": [],
   "source": [
    "diagnosis_breaddown_metadata = metadata.groupby(['diagnosis'], dropna=False, as_index=False).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c183b07-8de3-4fc6-9bc6-a5ced23e6f74",
   "metadata": {},
   "outputs": [],
   "source": [
    "benign_malignant_breaddown_metadata = metadata.groupby(['benign_malignant'], dropna=False, as_index=False).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4b1bb5e-3642-4608-984f-fb44b611f014",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "plt.rcParams['axes.titley'] = -0.7\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(23,5))\n",
    "\n",
    "# diagnosis plot\n",
    "data = diagnosis_breaddown_metadata['size']\n",
    "keys = diagnosis_breaddown_metadata['diagnosis']\n",
    "palette_color = ['#333745', '#E63462', '#FE5F55', '#C7EFCF', '#C7EFCF', '#D5573B', '#885053', '#777DA7', '#94C9A9', '#C6ECAE', '#3E92CC', '#2A628F', '#13293D', '#16324F', '#18435A', '#F4E04D', '#FCFCFC', '#54F2F2', '#5EB1BF', '#042A2B', '#EAC435', '#345995', '#03CEA4', '#FB4D3D', '#CA1551', '#89023E', '#CC7178', '#FFD9DA', '#C7D9B7']\n",
    "shuffle(palette_color)\n",
    "percentages = 100. * data / data.sum()\n",
    "\n",
    "# plotting data on chart\n",
    "patches, texts = ax1.pie(data, colors=palette_color, startangle=90, radius=2.5)\n",
    "labels = ['{0} - {1:1.2f} %'.format(i,j) for i,j in zip(keys, percentages)]\n",
    "patches, labels, dummy =  zip(*sorted(zip(patches, labels, data),\n",
    "                                      key=lambda x: x[2],\n",
    "                                      reverse=True))\n",
    "ax1.set_title('Pic.3. Metadata breakdown by diagnosis')\n",
    "ax1.legend(patches, labels, loc='upper right', bbox_to_anchor=(-0.1, 1.),\n",
    "           fontsize=10)\n",
    "\n",
    "# diagnosis plot\n",
    "data = benign_malignant_breaddown_metadata['size']\n",
    "keys = benign_malignant_breaddown_metadata['benign_malignant']\n",
    "palette_color = ['#333745', '#E63462', '#FE5F55', '#C7EFCF', '#C7EFCF', '#D5573B', '#885053', '#777DA7', '#94C9A9', '#C6ECAE', '#3E92CC', '#2A628F', '#13293D', '#16324F', '#18435A', '#F4E04D', '#FCFCFC', '#54F2F2', '#5EB1BF', '#042A2B', '#EAC435', '#345995', '#03CEA4', '#FB4D3D', '#CA1551', '#89023E', '#CC7178', '#FFD9DA', '#C7D9B7']\n",
    "shuffle(palette_color)\n",
    "percentages = 100. * data / data.sum()\n",
    "\n",
    "# plotting data on chart\n",
    "patches, texts = ax2.pie(data, colors=palette_color, startangle=90, radius=2.5)\n",
    "labels = ['{0} - {1:1.2f} %'.format(i,j) for i,j in zip(keys, percentages)]\n",
    "patches, labels, dummy =  zip(*sorted(zip(patches, labels, data),\n",
    "                                      key=lambda x: x[2],\n",
    "                                      reverse=True))\n",
    "\n",
    "ax2.set_title('Pic. 4. Metadata breakdown by malignancy')\n",
    "ax2.legend(patches, labels, loc='upper right', bbox_to_anchor=(-0.1, 1.),\n",
    "           fontsize=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d293084-c78b-4dda-9e98-b6419bf780d1",
   "metadata": {},
   "source": [
    "<a id='metadata_breakdowns'></a>\n",
    "From the first pie chart, we see, that __39.28%__ samples don't have a diagnosis. We think, that the fact somehow correlates with the lesions' malignancy and the way the diagnosis' were obtained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac1c225d-d453-4613-8453-69383a7fea3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "no_diagnosis_metadata = metadata[pd.isnull(metadata['diagnosis'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11150533-2786-4824-86a9-0b7e8c5ea4c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "benign_malignant_breaddown_no_diagnosis_metadata = no_diagnosis_metadata.groupby(['benign_malignant'], dropna=False, as_index=False).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f8f9420-357f-40dc-86e6-982a5746e1e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "diagnosis_confirm_type_breaddown_no_diagnosis_metadata = no_diagnosis_metadata.groupby(['diagnosis_confirm_type'], dropna=False, as_index=False).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67799fb0-fbca-4b71-9bed-205f68c18f29",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "plt.rcParams['axes.titley'] = -0.7\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(23,5))\n",
    "\n",
    "# diagnosis plot\n",
    "data = benign_malignant_breaddown_no_diagnosis_metadata['size']\n",
    "keys = benign_malignant_breaddown_no_diagnosis_metadata['benign_malignant']\n",
    "palette_color = ['#333745', '#E63462', '#FE5F55', '#C7EFCF', '#C7EFCF', '#D5573B', '#885053', '#777DA7', '#94C9A9', '#C6ECAE', '#3E92CC', '#2A628F', '#13293D', '#16324F', '#18435A', '#F4E04D', '#FCFCFC', '#54F2F2', '#5EB1BF', '#042A2B', '#EAC435', '#345995', '#03CEA4', '#FB4D3D', '#CA1551', '#89023E', '#CC7178', '#FFD9DA', '#C7D9B7']\n",
    "shuffle(palette_color)\n",
    "percentages = 100. * data / data.sum()\n",
    "\n",
    "# plotting data on chart\n",
    "patches, texts = ax1.pie(data, colors=palette_color, startangle=90, radius=2.5)\n",
    "labels = ['{0} - {1:1.2f} %'.format(i,j) for i,j in zip(keys, percentages)]\n",
    "patches, labels, dummy =  zip(*sorted(zip(patches, labels, data),\n",
    "                                      key=lambda x: x[2],\n",
    "                                      reverse=True))\n",
    "ax1.set_title('Pic. 5. Metadata with no dignosis breakdown by malignancy')\n",
    "ax1.legend(patches, labels, loc='upper right', bbox_to_anchor=(-0.1, 1.),\n",
    "           fontsize=10)\n",
    "\n",
    "# diagnosis plot\n",
    "data = diagnosis_confirm_type_breaddown_no_diagnosis_metadata['size']\n",
    "keys = diagnosis_confirm_type_breaddown_no_diagnosis_metadata['diagnosis_confirm_type']\n",
    "palette_color = ['#333745', '#E63462', '#FE5F55', '#C7EFCF', '#C7EFCF', '#D5573B', '#885053', '#777DA7', '#94C9A9', '#C6ECAE', '#3E92CC', '#2A628F', '#13293D', '#16324F', '#18435A', '#F4E04D', '#FCFCFC', '#54F2F2', '#5EB1BF', '#042A2B', '#EAC435', '#345995', '#03CEA4', '#FB4D3D', '#CA1551', '#89023E', '#CC7178', '#FFD9DA', '#C7D9B7']\n",
    "shuffle(palette_color)\n",
    "percentages = 100. * data / data.sum()\n",
    "\n",
    "# plotting data on chart\n",
    "patches, texts = ax2.pie(data, colors=palette_color, startangle=90, radius=2.5)\n",
    "labels = ['{0} - {1:1.2f} %'.format(i,j) for i,j in zip(keys, percentages)]\n",
    "patches, labels, dummy =  zip(*sorted(zip(patches, labels, data),\n",
    "                                      key=lambda x: x[2],\n",
    "                                      reverse=True))\n",
    "\n",
    "ax2.set_title('Pic. 6. Metadata with no dignosis breakdown by diagnosis confirmation type')\n",
    "ax2.legend(patches, labels, loc='upper right', bbox_to_anchor=(-0.1, 1.),\n",
    "           fontsize=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90583b27-0732-449b-99c8-b1a766afb744",
   "metadata": {},
   "source": [
    "__99.72%__ of samples without a diagnosis are benign and only __1.52%__ are consirmed with histopathology research. \n",
    "\n",
    "Probably, because excision wasn't done, the hospital protocol allowed not to put exact diagnosis. But imaging researches were enough to conclude that the lesions were benign.\n",
    "\n",
    "Let's see, what kind of research is made with the samples, which have diagnosis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "902111f6-3f1b-4650-94fb-adf68dd5ddef",
   "metadata": {},
   "outputs": [],
   "source": [
    "yes_diagnosis_metadata = metadata[pd.notnull(metadata['diagnosis'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac218531-dd7f-4c36-9752-8faee9782eeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "benign_malignant_breaddown_yes_diagnosis_metadata = yes_diagnosis_metadata.groupby(['benign_malignant'], dropna=False, as_index=False).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b524611a-29e0-47a4-bf74-d2d4b22e9c65",
   "metadata": {},
   "outputs": [],
   "source": [
    "diagnosis_confirm_type_breaddown_yes_diagnosis_metadata = yes_diagnosis_metadata.groupby(['diagnosis_confirm_type'], dropna=False, as_index=False).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ac330f7-74cb-47f3-b48b-320575603167",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "plt.rcParams['axes.titley'] = -0.7\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(23,5))\n",
    "\n",
    "# diagnosis plot\n",
    "data = benign_malignant_breaddown_yes_diagnosis_metadata['size']\n",
    "keys = benign_malignant_breaddown_yes_diagnosis_metadata['benign_malignant']\n",
    "palette_color = ['#333745', '#E63462', '#FE5F55', '#C7EFCF', '#C7EFCF', '#D5573B', '#885053', '#777DA7', '#94C9A9', '#C6ECAE', '#3E92CC', '#2A628F', '#13293D', '#16324F', '#18435A', '#F4E04D', '#FCFCFC', '#54F2F2', '#5EB1BF', '#042A2B', '#EAC435', '#345995', '#03CEA4', '#FB4D3D', '#CA1551', '#89023E', '#CC7178', '#FFD9DA', '#C7D9B7']\n",
    "shuffle(palette_color)\n",
    "percentages = 100. * data / data.sum()\n",
    "\n",
    "# plotting data on chart\n",
    "patches, texts = ax1.pie(data, colors=palette_color, startangle=90, radius=2.5)\n",
    "labels = ['{0} - {1:1.2f} %'.format(i,j) for i,j in zip(keys, percentages)]\n",
    "patches, labels, dummy =  zip(*sorted(zip(patches, labels, data),\n",
    "                                      key=lambda x: x[2],\n",
    "                                      reverse=True))\n",
    "ax1.set_title('Pic. 7. Metadata with no dignosis breakdown by malignancy')\n",
    "ax1.legend(patches, labels, loc='upper right', bbox_to_anchor=(-0.1, 1.),\n",
    "           fontsize=10)\n",
    "\n",
    "# diagnosis plot\n",
    "data = diagnosis_confirm_type_breaddown_yes_diagnosis_metadata['size']\n",
    "keys = diagnosis_confirm_type_breaddown_yes_diagnosis_metadata['diagnosis_confirm_type']\n",
    "palette_color = ['#333745', '#E63462', '#FE5F55', '#C7EFCF', '#C7EFCF', '#D5573B', '#885053', '#777DA7', '#94C9A9', '#C6ECAE', '#3E92CC', '#2A628F', '#13293D', '#16324F', '#18435A', '#F4E04D', '#FCFCFC', '#54F2F2', '#5EB1BF', '#042A2B', '#EAC435', '#345995', '#03CEA4', '#FB4D3D', '#CA1551', '#89023E', '#CC7178', '#FFD9DA', '#C7D9B7']\n",
    "shuffle(palette_color)\n",
    "percentages = 100. * data / data.sum()\n",
    "\n",
    "# plotting data on chart\n",
    "patches, texts = ax2.pie(data, colors=palette_color, startangle=90, radius=2.5)\n",
    "labels = ['{0} - {1:1.2f} %'.format(i,j) for i,j in zip(keys, percentages)]\n",
    "patches, labels, dummy =  zip(*sorted(zip(patches, labels, data),\n",
    "                                      key=lambda x: x[2],\n",
    "                                      reverse=True))\n",
    "\n",
    "ax2.set_title('Pic. 8. Metadata with no dignosis breakdown by diagnosis confirmation type')\n",
    "ax2.legend(patches, labels, loc='upper right', bbox_to_anchor=(-0.1, 1.),\n",
    "           fontsize=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3cebd33-bc1c-4ee1-a89b-80f0f7943c90",
   "metadata": {},
   "source": [
    "Out of the samples with diagnosis, much more of them were confirmed with a histopathological research - __48.18%__. But, and that's important, if a sample has a diagnosis, it doesn't mean, that it was confirmed with a histopathological research."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a476e8ea-5730-49ea-b725-f49454d9f5f9",
   "metadata": {},
   "source": [
    "Let's go back to the [Pic. 4. Breakdown by malignancy](#metadata_breakdowns) . __10.92%__ of samples have unknown malignancy, so we will study it through diagnosis and other columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "191c110b-d2f8-4353-94e9-58e519154707",
   "metadata": {},
   "outputs": [],
   "source": [
    "no_benign_malignant_metadata = metadata[pd.isnull(metadata['benign_malignant'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25dcacbc-1be2-4449-ae71-f48ffa0c09dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "diagnosis_breakdown_no_benign_malignant_metadata = no_benign_malignant_metadata.groupby(['diagnosis'], dropna=False, as_index=False).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bb6a849-98a3-4c27-b010-d376141b6f80",
   "metadata": {},
   "outputs": [],
   "source": [
    "diagnosis_confirm_type_no_benign_malignant_metadata = no_benign_malignant_metadata.groupby(['diagnosis_confirm_type'], dropna=False, as_index=False).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "045a3b83-0edc-48c0-95d3-ade827ec48c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "plt.rcParams['axes.titley'] = -0.7\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(23,5))\n",
    "\n",
    "# diagnosis plot\n",
    "data = diagnosis_breakdown_no_benign_malignant_metadata['size']\n",
    "keys = diagnosis_breakdown_no_benign_malignant_metadata['diagnosis']\n",
    "palette_color = ['#333745', '#E63462', '#FE5F55', '#C7EFCF', '#C7EFCF', '#D5573B', '#885053', '#777DA7', '#94C9A9', '#C6ECAE', '#3E92CC', '#2A628F', '#13293D', '#16324F', '#18435A', '#F4E04D', '#FCFCFC', '#54F2F2', '#5EB1BF', '#042A2B', '#EAC435', '#345995', '#03CEA4', '#FB4D3D', '#CA1551', '#89023E', '#CC7178', '#FFD9DA', '#C7D9B7']\n",
    "shuffle(palette_color)\n",
    "percentages = 100. * data / data.sum()\n",
    "\n",
    "# plotting data on chart\n",
    "patches, texts = ax1.pie(data, colors=palette_color, startangle=90, radius=2.5)\n",
    "labels = ['{0} - {1:1.2f} %'.format(i,j) for i,j in zip(keys, percentages)]\n",
    "patches, labels, dummy =  zip(*sorted(zip(patches, labels, data),\n",
    "                                      key=lambda x: x[2],\n",
    "                                      reverse=True))\n",
    "ax1.set_title('Pic. 9. Metadata with no malignancy breakdown by diagnosis')\n",
    "ax1.legend(patches, labels, loc='upper right', bbox_to_anchor=(-0.1, 1.),\n",
    "           fontsize=10)\n",
    "\n",
    "# diagnosis plot\n",
    "data = diagnosis_confirm_type_no_benign_malignant_metadata['size']\n",
    "keys = diagnosis_confirm_type_no_benign_malignant_metadata['diagnosis_confirm_type']\n",
    "palette_color = ['#333745', '#E63462', '#FE5F55', '#C7EFCF', '#C7EFCF', '#D5573B', '#885053', '#777DA7', '#94C9A9', '#C6ECAE', '#3E92CC', '#2A628F', '#13293D', '#16324F', '#18435A', '#F4E04D', '#FCFCFC', '#54F2F2', '#5EB1BF', '#042A2B', '#EAC435', '#345995', '#03CEA4', '#FB4D3D', '#CA1551', '#89023E', '#CC7178', '#FFD9DA', '#C7D9B7']\n",
    "shuffle(palette_color)\n",
    "percentages = 100. * data / data.sum()\n",
    "\n",
    "# plotting data on chart\n",
    "patches, texts = ax2.pie(data, colors=palette_color, startangle=90, radius=2.5)\n",
    "labels = ['{0} - {1:1.2f} %'.format(i,j) for i,j in zip(keys, percentages)]\n",
    "patches, labels, dummy =  zip(*sorted(zip(patches, labels, data),\n",
    "                                      key=lambda x: x[2],\n",
    "                                      reverse=True))\n",
    "\n",
    "ax2.set_title('Pic. 10. Metadata with no malignancy breakdown by diagnosis confirmation type')\n",
    "ax2.legend(patches, labels, loc='upper right', bbox_to_anchor=(-0.1, 1.),\n",
    "           fontsize=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b76baed5-181f-42ea-8240-08e689e86bc1",
   "metadata": {},
   "source": [
    "For the most cases, the issues is with incorrectly filled data:\n",
    "- [Basal-cell carcinoma](https://en.wikipedia.org/wiki/Basal-cell_carcinoma) - malignant\n",
    "- Pigmented benign ketosis - benign\n",
    "- [Seborrheic keratosis](https://en.wikipedia.org/wiki/Seborrheic_keratosis) - benign\n",
    "- [Actinic keratosis](https://en.wikipedia.org/wiki/Actinic_keratosis) - precancerous, untreated lesions have up to a 20% risk of progression to squamous cell carcinoma, for our purposes, we'll consider it malignant\n",
    "- [Squamous cell carcinoma](https://en.wikipedia.org/wiki/Squamous_cell_carcinoma) - malignant\n",
    "- [Vascular lesion](https://en.wikipedia.org/wiki/Vascular_anomaly) - benign\n",
    "- [Dermatofibroma](https://en.wikipedia.org/wiki/Dermatofibroma) - benign\n",
    "- [Solar lentigo](https://en.wikipedia.org/wiki/Liver_spot) - benign\n",
    "\n",
    "Let's fix the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cf328c8-8006-4aff-9402-3cc21e60f113",
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata.loc[metadata['diagnosis'] == 'basal cell carcinoma', 'benign_malignant']  = 'malignant'\n",
    "metadata.loc[metadata['diagnosis'] == 'pigmented benign keratosis', 'benign_malignant']  = 'benign'\n",
    "metadata.loc[metadata['diagnosis'] == 'seborrheic keratosis', 'benign_malignant']  = 'benign'\n",
    "metadata.loc[metadata['diagnosis'] == 'actinic keratosis', 'benign_malignant']  = 'malignant'\n",
    "metadata.loc[metadata['diagnosis'] == 'squamous cell carcinoma', 'benign_malignant']  = 'malignant'\n",
    "metadata.loc[metadata['diagnosis'] == 'vascular lesion', 'benign_malignant']  = 'benign'\n",
    "metadata.loc[metadata['diagnosis'] == 'dermatofibroma', 'benign_malignant']  = 'benign'\n",
    "metadata.loc[metadata['diagnosis'] == 'solar lentigo', 'benign_malignant']  = 'benign'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae35cc6a-2b28-4fb4-b3e3-2219003ce6ab",
   "metadata": {},
   "source": [
    "Returning to samples without a diagnosis, we have two ways out of this:\n",
    "- remove the samples\n",
    "- introduce a new \"label\"\n",
    "\n",
    "We don't know much about those samples except that they contain all kind of benign skin conditions. So, for example, there will be __solar lentigo__ samples in the __solar lentigo__ subset and the undiagnosed subset. That will may lead to classifying __solar lentigo__ as a part of undiagnosed subset. That's why, it's better to exclude the undiagnosed samples from training.\n",
    "\n",
    "In addition to that, the dataset is tilted toward benign skin conditions, and almost all undiagnosed samples represent benign skin conditions. In that way, we not only remove confusing data but perform undersampling to balance out the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f67f3d7c-1b35-4ea3-b082-5c55583807e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata = metadata.drop(metadata[pd.isnull(metadata['diagnosis'])].index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aa819dc-0894-4359-a501-56369a0f907c",
   "metadata": {},
   "source": [
    "Later on we will need to split the dataset into training, validation and testing. To do that, all diagnoses should have sufficient amount of  samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aba1e9e-51eb-4135-9d70-a0216b37523a",
   "metadata": {},
   "outputs": [],
   "source": [
    "diagnosis_numbers = metadata.groupby(['diagnosis'], dropna=False, as_index=False).size().sort_values(by='size')\n",
    "diagnosis_numbers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab372436-f8ee-471e-b14a-ed2f6a5d9e86",
   "metadata": {},
   "source": [
    "We'll remove the samples, which are not part of the 10 most common diagnosis categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04f7db90-d19b-45ac-81a6-8538bdaa6a3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "rare_diagnosis = diagnosis_numbers[:-10]['diagnosis']\n",
    "rare_diagnosis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28cdb64a-f876-4ff3-abdb-319067cd1b23",
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata = metadata.drop(metadata[metadata['diagnosis'].isin(rare_diagnosis)].index)\n",
    "metadata = metadata.drop(metadata[~metadata['benign_malignant'].isin(['benign', 'malignant'])].index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfb4dd5d-50e6-4eef-88ba-1c77040d9b95",
   "metadata": {},
   "source": [
    "Let's look at the breakdown now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97394c37-975a-4c9d-8212-9f07e0d5a966",
   "metadata": {},
   "outputs": [],
   "source": [
    "diagnosis_breaddown_metadata = metadata.groupby(['diagnosis'], dropna=False, as_index=False).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "383926c4-11e5-4e1e-a934-310aca60be68",
   "metadata": {},
   "outputs": [],
   "source": [
    "benign_malignant_breaddown_metadata = metadata.groupby(['benign_malignant'], dropna=False, as_index=False).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bfd8541-9850-4947-b2a2-aa1220acbeca",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "plt.rcParams['axes.titley'] = -0.7\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(23,5))\n",
    "\n",
    "# diagnosis plot\n",
    "data = diagnosis_breaddown_metadata['size']\n",
    "keys = diagnosis_breaddown_metadata['diagnosis']\n",
    "palette_color = ['#333745', '#E63462', '#FE5F55', '#C7EFCF', '#C7EFCF', '#D5573B', '#885053', '#777DA7', '#94C9A9', '#C6ECAE', '#3E92CC', '#2A628F', '#13293D', '#16324F', '#18435A', '#F4E04D', '#FCFCFC', '#54F2F2', '#5EB1BF', '#042A2B', '#EAC435', '#345995', '#03CEA4', '#FB4D3D', '#CA1551', '#89023E', '#CC7178', '#FFD9DA', '#C7D9B7']\n",
    "shuffle(palette_color)\n",
    "percentages = 100. * data / data.sum()\n",
    "\n",
    "# plotting data on chart\n",
    "patches, texts = ax1.pie(data, colors=palette_color, startangle=90, radius=2.5)\n",
    "labels = ['{0} - {1:1.2f} %'.format(i,j) for i,j in zip(keys, percentages)]\n",
    "patches, labels, dummy =  zip(*sorted(zip(patches, labels, data),\n",
    "                                      key=lambda x: x[2],\n",
    "                                      reverse=True))\n",
    "ax1.set_title('Pic.11. Metadata breakdown by diagnosis after preprocessing')\n",
    "ax1.legend(patches, labels, loc='upper right', bbox_to_anchor=(-0.1, 1.),\n",
    "           fontsize=10)\n",
    "\n",
    "# diagnosis plot\n",
    "data = benign_malignant_breaddown_metadata['size']\n",
    "keys = benign_malignant_breaddown_metadata['benign_malignant']\n",
    "palette_color = ['#333745', '#E63462', '#FE5F55', '#C7EFCF', '#C7EFCF', '#D5573B', '#885053', '#777DA7', '#94C9A9', '#C6ECAE', '#3E92CC', '#2A628F', '#13293D', '#16324F', '#18435A', '#F4E04D', '#FCFCFC', '#54F2F2', '#5EB1BF', '#042A2B', '#EAC435', '#345995', '#03CEA4', '#FB4D3D', '#CA1551', '#89023E', '#CC7178', '#FFD9DA', '#C7D9B7']\n",
    "percentages = 100. * data / data.sum()\n",
    "\n",
    "# plotting data on chart\n",
    "patches, texts = ax2.pie(data, colors=palette_color, startangle=90, radius=2.5)\n",
    "labels = ['{0} - {1:1.2f} %'.format(i,j) for i,j in zip(keys, percentages)]\n",
    "patches, labels, dummy =  zip(*sorted(zip(patches, labels, data),\n",
    "                                      key=lambda x: x[2],\n",
    "                                      reverse=True))\n",
    "\n",
    "ax2.set_title('Pic. 12. Metadata breakdown by malignancy after preprocessing')\n",
    "ax2.legend(patches, labels, loc='upper right', bbox_to_anchor=(-0.1, 1.),\n",
    "           fontsize=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4443bc4f-e761-4727-80e3-1cb4b20e17dd",
   "metadata": {},
   "source": [
    "The dataset is still imbalanced, so we'll calculate class weights to use them in training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "574f8faa-215d-4e4d-a402-b266f31c36f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_weights = metadata.groupby(['diagnosis'], dropna=False, as_index=False).size()\n",
    "class_weights.loc[:, 'size'] =  1 / class_weights.loc[:, 'size']\n",
    "class_weights.loc[:, 'size'] *= ( len(metadata) / 10 )\n",
    "class_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bc821ba-8f59-48c3-ad4d-12e46ebe0e23",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_weights = dict(enumerate(class_weights['size']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f77363b-3fb3-4d57-822c-5759f82f26d2",
   "metadata": {},
   "source": [
    "We transform ISIC ids to filenames. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67dee4d0-5112-47dc-95ea-38633664eb19",
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata['isic_id'] += '.jpg'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e8ac41a-ca11-4b70-bf0d-d3d32dc66038",
   "metadata": {},
   "source": [
    "We need to extract testing files from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84828bbd-80b2-46ca-9062-4a1eb5530bc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "rest_set, testing_set = train_test_split(metadata, test_size=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82d5555b-f6e0-424d-b796-44e650f5408c",
   "metadata": {},
   "source": [
    "Finally, we can load the images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3826b89e-5302-414a-b22a-8e4d4be88676",
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = tf.keras.preprocessing.image.ImageDataGenerator(\n",
    "    rotation_range=180,\n",
    "    horizontal_flip=True,\n",
    "    vertical_flip=True,\n",
    "    brightness_range=(0.2, 1.5),\n",
    "    validation_split=0.2\n",
    ")\n",
    "\n",
    "training_set = generator.flow_from_dataframe(\n",
    "    rest_set,\n",
    "    directory=f\"{os.environ['SCRATCH']}/isic-archive\",\n",
    "    x_col='isic_id',\n",
    "    y_col='diagnosis',\n",
    "    \n",
    "    target_size=(299, 299), \n",
    "    batch_size=32, \n",
    "    class_mode='categorical', \n",
    "    subset='training'\n",
    ")\n",
    "\n",
    "validation_set = generator.flow_from_dataframe(\n",
    "    rest_set,\n",
    "    directory=f\"{os.environ['SCRATCH']}/isic-archive\",\n",
    "    x_col='isic_id',\n",
    "    y_col='diagnosis',\n",
    "    \n",
    "    target_size=(299, 299), \n",
    "    batch_size=32, \n",
    "    class_mode='categorical', \n",
    "    subset='validation'\n",
    ")\n",
    "\n",
    "testing_set = generator.flow_from_dataframe(\n",
    "    testing_set,\n",
    "    directory=f\"{os.environ['SCRATCH']}/isic-archive\",\n",
    "    x_col='isic_id',\n",
    "    y_col='diagnosis',\n",
    "    \n",
    "    target_size=(299, 299), \n",
    "    batch_size=32, \n",
    "    class_mode='categorical'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b565f0a-751a-408e-aae1-0d9be6ddfe14",
   "metadata": {},
   "outputs": [],
   "source": [
    "CLASS_NUMBER = len(training_set.class_indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "587532e5-fb37-49f7-b49e-b0a5fb2deea9",
   "metadata": {},
   "source": [
    "## Data source"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca6d36e2-16ad-4870-b71f-1ff29f70683d",
   "metadata": {},
   "source": [
    "As a data source, we use the ISIC Archive."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71d8c9b6-6b22-4041-b0cf-c1dfc64d029c",
   "metadata": {},
   "source": [
    "The ISIC Archive is an open source platform with publicly available images of skin lesions under Creative Commons licenses. The images are associated with ground-truth diagnoses and other clinical metadata. Images can be queried using faceted search and downloaded individually or in batches. The initial focus of the archive has been on dermoscopy images of individual skin lesions, as these images are inherently standardized by the use of a specialized acquisition device and devoid of many of the privacy challenges associated with clinical images. To date, the images have been provided by specialized melanoma centers from around the world. The archive is designed to accept contributions from new sources under the Terms of Use and welcomes new contributors. There are ongoing efforts to supplement the dermoscopy images in the archive with close-up clinical images and a broader representation of skin types. The images in the Archive are used to support educational efforts through linkage with Dermoscopedia and are used for Grand Challenges and Live Challenges to engage the computer science community for the development of diagnostic AI."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6dc1a5b-96bb-4385-a41e-2e60a7ef72c9",
   "metadata": {},
   "source": [
    "For more information, go to [ISIC Archive web site](https://www.isic-archive.com/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bd8a559-b7dc-4847-bdcd-339d192fe120",
   "metadata": {},
   "source": [
    "# Model training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec12c2e1-4808-48b1-bafe-54b07fd88686",
   "metadata": {},
   "source": [
    "## Preparing TensorFlow callbacks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d74b9e4-0acd-41a8-b55a-12e85594fd39",
   "metadata": {},
   "source": [
    "For our convenience, we create a few TensorFlow callbacks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d26734d-1e26-41f7-8e6b-5d36789ac1d7",
   "metadata": {},
   "source": [
    "### The EarlyStopping callback"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e53cb101-1001-4020-94bf-8eb10db2fb0a",
   "metadata": {},
   "source": [
    "This callback stops training when the metrics (e.g. validation loss) are not improving,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "249d8b51-a908-45e3-8199-a3f3df18ea2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stop_callback = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor=\"val_loss\", \n",
    "    min_delta=0.01, \n",
    "    patience=5, \n",
    "    restore_best_weights=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad2f4daa-fb80-4fdf-9ced-aa8a5e5c427e",
   "metadata": {},
   "source": [
    "### The ModelCheckpoint callback"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0e6edc3-b255-4864-a2d2-17f0b3a8b837",
   "metadata": {},
   "source": [
    "This callback saves the model with the best metrics during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bda2e349-bed4-4b55-8c7d-7cfbb0a230f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path = 'checkpoints/model.ckpt'\n",
    "\n",
    "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    checkpoint_path,\n",
    "    monitor='val_loss',\n",
    "    verbose=1,\n",
    "    save_best_only=True,\n",
    "    save_weights_only=False,\n",
    "    save_freq='epoch',\n",
    "    mode='auto'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09181566-32a2-4bb6-b3cf-dcf558ae0fd7",
   "metadata": {},
   "source": [
    "## Define the distribution strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4ae60fc-5368-4bd9-84d8-ace581b65851",
   "metadata": {},
   "outputs": [],
   "source": [
    "strategy = tf.distribute.MirroredStrategy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dc48599-1daa-4115-b60d-14fdcfc71708",
   "metadata": {},
   "source": [
    "## Actual training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a55b6142-7459-49bc-bb93-d163ffb197b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "with strategy.scope():\n",
    "    model = FractalNeuralNetwork(class_number=CLASS_NUMBER)\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9db63e4-3478-4dc2-864e-86538aef11ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(\n",
    "    training_set, \n",
    "    validation_data=validation_set, \n",
    "    epochs=30,\n",
    "    callbacks=[\n",
    "        early_stop_callback,\n",
    "        checkpoint_callback\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c00b2eda-14c1-4ad9-8c3d-d8fea9fc7a6e",
   "metadata": {},
   "source": [
    "# Model validation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3078a520-e94b-42a6-95cc-9df4fbc28f8f",
   "metadata": {},
   "source": [
    "## Loading the model from the checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f068b5d6-e3b7-4f33-81fe-77eb1e64e528",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = FractalNeuralNetwork(class_number=CLASS_NUMBER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59b32593-5593-44ab-a04c-6880fd0babc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights(checkpoint_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1e37ac9-5ce7-49f6-99da-783dcd5e5890",
   "metadata": {},
   "source": [
    "## Making diagnoses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3112ab04-08a2-4669-bdb1-891606a1fc06",
   "metadata": {},
   "outputs": [],
   "source": [
    "true_labels = np.concatenate([testing_set[i][1] for i in range(len(testing_set))], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a69e04d8-a46f-4735-a6ec-0cad41eac88a",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_labels = model.predict(testing_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad2c91e5-525b-4046-89ce-a554656d752f",
   "metadata": {},
   "source": [
    "## Plot the ROC Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df46bcb3-7d5c-4c44-bea6-145cc70c775e",
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr = dict()\n",
    "tpr = dict()\n",
    "auc_metric = dict()\n",
    "\n",
    "diagnosis_index_dict = {v: k for k, v in testing_set.class_indices.items()}\n",
    "\n",
    "for i in range(len(diagnosis_index_dict)):\n",
    "    diagnosis = diagnosis_index_dict[i]\n",
    "    fpr[diagnosis], tpr[diagnosis], _ = roc_curve(true_labels[:, i], predicted_labels[:, i])\n",
    "    auc_metric[diagnosis] = auc(fpr[diagnosis], tpr[diagnosis])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf661314-c25e-43f3-9417-dfd321e78e5d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%matplotlib inline \n",
    "for diagnosis in testing_set.class_indices:\n",
    "    plt.plot(fpr[diagnosis], tpr[diagnosis], label=diagnosis)\n",
    "    \n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver operating characteristic')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c7c7a10-b9af-43cc-ab95-294b2a7118bf",
   "metadata": {},
   "source": [
    "## Show AUC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0ccc958-1069-4c2d-9c52-676c175878c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "auc_metric"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "131a4349-2a3f-45ba-82fc-b5205878a867",
   "metadata": {},
   "source": [
    "# Model explanation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af7c8b41-e215-485a-b35c-b669730c3c40",
   "metadata": {},
   "source": [
    "Now we want some explanations from out model.\n",
    "\n",
    "We load an image with melanoma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d707021-6393-4d5c-8b43-f7f3a55af487",
   "metadata": {},
   "outputs": [],
   "source": [
    "melanoma_im_path = f\"{os.environ['SCRATCH']}/isic-archive/ISIC_0000031.jpg\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a5602a0-af42-466f-bd28-cdaa55e45402",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Image(melanoma_im_path, width=600))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "737058b3-4953-4a7a-9812-28e9d178a0e1",
   "metadata": {},
   "source": [
    "Pic. 14. Image with melanoma."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffa34ed5-8195-40d9-b227-45a7c397e03b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_img_array(img_path, size):\n",
    "    img = tf.keras.preprocessing.image.load_img(img_path, target_size=size)\n",
    "    array = tf.keras.preprocessing.image.img_to_array(img)\n",
    "    array = np.expand_dims(array, axis=0)\n",
    "    return array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "098fa19f-fdcd-4e5c-b5c3-f4be18deaa7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_array = tf.keras.applications.inception_resnet_v2.preprocess_input(get_img_array(melanoma_im_path, size=(299, 299)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4875f229-6690-419a-adb4-a78b20dd1448",
   "metadata": {},
   "source": [
    "and plot Grad-CAM over the melanoma image,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1f97ea3-ee1f-41b0-94b5-64e3b76157bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_gradcam_heatmap(img_array, model, last_conv_layer_name, pred_index=None):\n",
    "    grad_model = tf.keras.models.Model(\n",
    "        [model.inputs], [model.get_layer(last_conv_layer_name).output, model.output]\n",
    "    )\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        last_conv_layer_output, preds = grad_model(img_array)\n",
    "        if pred_index is None:\n",
    "            pred_index = tf.argmax(preds[0])\n",
    "        class_channel = preds[:, pred_index]\n",
    "\n",
    "    grads = tape.gradient(class_channel, last_conv_layer_output)\n",
    "    \n",
    "    pooled_grads = tf.reduce_mean(grads, axis=(0, 1, 2))\n",
    "\n",
    "    last_conv_layer_output = last_conv_layer_output[0]\n",
    "    heatmap = last_conv_layer_output @ pooled_grads[..., tf.newaxis]\n",
    "    heatmap = tf.squeeze(heatmap)\n",
    "\n",
    "    heatmap = tf.maximum(heatmap, 0) / tf.math.reduce_max(heatmap)\n",
    "    return heatmap.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f48c2ae7-f003-4b1e-9c98-68ddcd70d61b",
   "metadata": {},
   "outputs": [],
   "source": [
    "heatmap = make_gradcam_heatmap(img_array, model.original_model, 'conv_7b')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdd73af3-13bd-4ca4-b4e4-e7dbf1ecc349",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_gradcam(img_path, heatmap, alpha=0.4):\n",
    "    img = tf.keras.preprocessing.image.load_img(img_path)\n",
    "    img = tf.keras.preprocessing.image.img_to_array(img)\n",
    "    \n",
    "    heatmap = np.uint8(255 * heatmap)\n",
    "    \n",
    "    jet = cm.get_cmap(\"jet\")\n",
    "    \n",
    "    jet_colors = jet(np.arange(256))[:, :3]\n",
    "    jet_heatmap = jet_colors[heatmap]\n",
    "    \n",
    "    jet_heatmap = tf.keras.preprocessing.image.array_to_img(jet_heatmap)\n",
    "    jet_heatmap = jet_heatmap.resize((img.shape[1], img.shape[0]))\n",
    "    jet_heatmap = tf.keras.preprocessing.image.img_to_array(jet_heatmap)\n",
    "    \n",
    "    superimposed_img = jet_heatmap * alpha + img\n",
    "    superimposed_img = tf.keras.preprocessing.image.array_to_img(superimposed_img)\n",
    "    superimposed_img = superimposed_img.resize((np.array(superimposed_img.size) * 600 / 1504).astype(int))\n",
    "\n",
    "\n",
    "    display(superimposed_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e0aac19-c947-4315-8630-2c0b43ba4491",
   "metadata": {},
   "outputs": [],
   "source": [
    "display_gradcam(melanoma_im_path, heatmap)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8b92dfd-79c0-497f-bec5-1554c636267c",
   "metadata": {},
   "source": [
    "Pic. 15. Grad-CAM image of the melanoma."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2e6fdb6-0eef-408f-807f-0427fe67d93a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
