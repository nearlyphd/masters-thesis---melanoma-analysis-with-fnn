{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "overall-procedure",
   "metadata": {},
   "source": [
    "# Melanoma analysis with fractal neural networks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "local-chair",
   "metadata": {},
   "source": [
    "This notebook shows how good is [Fractal neural network](#Fractal-neural-network) for [melanoma](#Melanoma) analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bridal-andrews",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow_addons as tfa\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_curve, auc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "level-adelaide",
   "metadata": {},
   "source": [
    "Check if a GPU is available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "adult-granny",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.config.list_physical_devices('GPU')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "demonstrated-bidding",
   "metadata": {},
   "source": [
    "Remove excessive logging. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "frequent-mortality",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.get_logger().setLevel('ERROR')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "frozen-contemporary",
   "metadata": {},
   "source": [
    "# Melanoma"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "distinguished-parliament",
   "metadata": {},
   "source": [
    "__Melanoma__, also redundantly known as __malignant melanoma__, is a type of skin cancer that develops from the pigment-producing cells known as melanocytes. Melanomas typically occur in the skin, but may rarely occur in the mouth, intestines, or eye (uveal melanoma). In women, they most commonly occur on the legs, while in men, they most commonly occur on the back. About 25% of melanomas develop from moles. Changes in a mole that can indicate melanoma include an increase in size, irregular edges, change in color, itchiness, or skin breakdown."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "primary-desert",
   "metadata": {},
   "source": [
    "![melanoma image](../assets/melanoma.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "proper-andorra",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center; font-weight: bold\">Pic.1. A melanoma of approximately 2.5 cm (1 in) by 1.5 cm (0.6 in)</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "massive-compiler",
   "metadata": {},
   "source": [
    "The primary cause of melanoma is ultraviolet light (UV) exposure in those with low levels of the skin pigment melanin. The UV light may be from the sun or other sources, such as tanning devices. Those with many moles, a history of affected family members, and poor immune function are at greater risk. A number of rare genetic conditions, such as xeroderma pigmentosum, also increase the risk. Diagnosis is by biopsy and analysis of any skin lesion that has signs of being potentially cancerous."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "decreased-coverage",
   "metadata": {},
   "source": [
    "Melanoma is the most dangerous type of skin cancer. Globally, in 2012, it newly occurred in 232,000 people. In 2015, 3.1 million people had active disease, which resulted in 59,800 deaths. Australia and New Zealand have the highest rates of melanoma in the world. High rates also occur in Northern Europe and North America, while it is less common in Asia, Africa, and Latin America. In the United States, melanoma occurs about 1.6 times more often in men than women. Melanoma has become more common since the 1960s in areas mostly populated by people of European descent."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sixth-profession",
   "metadata": {},
   "source": [
    "# Fractal neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "differential-edwards",
   "metadata": {},
   "source": [
    "We propose an ensemble model based on handcrafted fractal features and deep learning that consists of combining the classification of two CNNs by applying the sum rule. We apply feature extraction to obtain 300 fractal features from different\n",
    "dermoscopy datasets. These features are reshaped into a 10 √ó 10 √ó 3 matrix to compose an artificial image that\n",
    "is given as input to the first CNN. The second CNN model receives as input the correspondent original image."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "delayed-racing",
   "metadata": {},
   "source": [
    "![CNN image](../assets/fnn.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dress-serial",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center; font-weight: bold\">Pic.2. Overview of the proposed FNN model.</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "spread-rolling",
   "metadata": {},
   "source": [
    "If you want to learn more about fractal neural networks, read [here](https://www.sciencedirect.com/science/article/abs/pii/S0957417420308563)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "smaller-session",
   "metadata": {},
   "source": [
    "## Dividing images into patches."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "married-immigration",
   "metadata": {},
   "source": [
    "According to the acticle:\n",
    "> One of the approaches available in the literature for multiscale analysis is the gliding-box algorithm (Ivanovici & Richard, 2011). The main advantage of this approach is that it can be applied on datasets containing images with different resolutions since the output features are given in relation to the scale instead of being absolute values. This algorithm consists in placing a box $\\beta_{i}$ sized $ùêø √ó ùêø$ on the left superior corner of the image, wherein ùêø is given in pixels. This box glides through the image, one column and then one row at a time. After reaching the end of the image, the box is repositioned at the starting point and the value of ùêø is increased by 2."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "intelligent-essex",
   "metadata": {},
   "source": [
    "The gliding-box method will not be used since it consumes too much RAM. We'll employ a box-counting approach, which basically means we'll partition the images into non-overlapping chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "funny-incidence",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Patchify(tf.keras.layers.Layer):\n",
    "    def __init__(self, patch_size):\n",
    "        super(Patchify, self).__init__()\n",
    "        \n",
    "        self.patch_size = patch_size\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        outputs = tf.image.extract_patches(\n",
    "            inputs,\n",
    "            sizes=(1, self.patch_size, self.patch_size, 1),\n",
    "            strides=(1, self.patch_size, self.patch_size, 1),\n",
    "            rates=(1, 1, 1, 1),\n",
    "            padding='SAME'\n",
    "        )\n",
    "        \n",
    "        _, rows, cols, _ = tf.unstack(tf.shape(outputs))\n",
    "        outputs = tf.reshape(outputs, shape=(-1, rows * cols, self.patch_size, self.patch_size, 3))\n",
    "        \n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "valid-contributor",
   "metadata": {},
   "source": [
    "## Creating an array of binary values from image patches using the Chebyshev colour distance function applied to the patch centre and each pixel."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "searching-assignment",
   "metadata": {},
   "source": [
    "According to the article:\n",
    "> For each time the box $\\beta_{i}$ is moved, a multidimensional analysis of colour similarity is performed for every pixel inside it. This is done by assigning the centre pixel to a vector $ùëì_{c} = ùëü_{c}, ùëî_{c}, ùëè_{c}$, where $ùëü_{c}, ùëî_{c}$ and $ùëè_{c}$ correspond to the colour intensities for each of the RGB colour channels of given pixel. The other pixels in the box are assigned to a vector $ùëì_{i} = ùëü_{i}, ùëî_{i}, ùëè_{i}$ and compared to the centre pixel by calculating a colour distance $\\Delta$. On the proposed approach, the Chebyshev ($\\Delta_{h}$) ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "combined-title",
   "metadata": {},
   "source": [
    "The following equation is used to compute the Chebyshev distance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "based-county",
   "metadata": {},
   "source": [
    "$$\n",
    "\\Delta_{h} = max(|f_{i}(k_{i}) - f_{c}(k_{c})|), k \\in r, g, b. \n",
    "$$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "built-poker",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Chebyshev(tf.keras.layers.Layer):\n",
    "    def __init__(self):\n",
    "        super(Chebyshev, self).__init__()\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        batch_size, patch_number, patch_size, patch_size, channels = tf.unstack(tf.shape(inputs))\n",
    "        outputs = tf.reshape(inputs, shape=(-1, patch_number, patch_size, channels))    \n",
    "        \n",
    "        centers = tf.image.resize_with_crop_or_pad(outputs, 1, 1)\n",
    "\n",
    "        outputs = tf.math.subtract(outputs, centers)\n",
    "        outputs = tf.math.abs(outputs)\n",
    "        outputs = tf.math.reduce_max(outputs, axis=3)\n",
    "        outputs = tf.math.less_equal(outputs, tf.cast(patch_size, dtype=tf.float32))\n",
    "        outputs = tf.cast(outputs, dtype=tf.int32)\n",
    "        outputs = tf.reshape(outputs, shape=(-1, patch_number, patch_size, patch_size))\n",
    "        \n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "legislative-brief",
   "metadata": {},
   "source": [
    "## Creating an array of binary values from image patches using the Euclidean colour distance function applied to the patch centre and each pixel."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "weird-emperor",
   "metadata": {},
   "source": [
    "According to the article:\n",
    "> For each time the box $\\beta_{i}$ is moved, a multidimensional analysis of colour similarity is performed for every pixel inside it. This is done by assigning the centre pixel to a vector $ùëì_{c} = ùëü_{c}, ùëî_{c}, ùëè_{c}$, where $ùëü_{c}, ùëî_{c}$ and $ùëè_{c}$ correspond to the colour intensities for each of the RGB colour channels of given pixel. The other pixels in the box are assigned to a vector $ùëì_{i} = ùëü_{i}, ùëî_{i}, ùëè_{i}$ and compared to the centre pixel by calculating a colour distance $\\Delta$. On the proposed approach, ... the Euclidean ($\\Delta_{e}$) ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "perfect-jones",
   "metadata": {},
   "source": [
    "$$\n",
    "\\Delta_{e} = \\sqrt{\\sum_{k} (f_{i}(k_{i}) - f_{c}(k_{c}))^2}, k \\in r, g, b\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "noted-passing",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Euclidean(tf.keras.layers.Layer):\n",
    "    def __init__(self):\n",
    "        super(Euclidean, self).__init__()\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        batch_size, patch_number, patch_size, patch_size, channels = tf.unstack(tf.shape(inputs))\n",
    "        outputs = tf.reshape(inputs, shape=(-1, patch_number, patch_size, channels))\n",
    "        \n",
    "        centers = tf.image.resize_with_crop_or_pad(outputs, 1, 1)\n",
    "\n",
    "        outputs = tf.math.subtract(outputs, centers)\n",
    "        outputs = tf.math.pow(outputs, 2)\n",
    "        outputs = tf.math.reduce_sum(outputs, axis=3)\n",
    "        outputs = tf.math.pow(outputs, 0.5)\n",
    "        outputs = tf.math.less_equal(outputs, tf.cast(patch_size, dtype=tf.float32))\n",
    "        outputs = tf.cast(outputs, dtype=tf.int32)\n",
    "        outputs = tf.reshape(outputs, shape=(-1, patch_number, patch_size, patch_size))\n",
    "        \n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "simple-damages",
   "metadata": {},
   "source": [
    "## Creating an array of binary values from image patches using the Manhattan colour distance function applied to the patch centre and each pixel."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "presidential-crest",
   "metadata": {},
   "source": [
    "According to the article:\n",
    "> For each time the box $\\beta_{i}$ is moved, a multidimensional analysis of colour similarity is performed for every pixel inside it. This is done by assigning the centre pixel to a vector $ùëì_{c} = ùëü_{c}, ùëî_{c}, ùëè_{c}$, where $ùëü_{c}, ùëî_{c}$ and $ùëè_{c}$ correspond to the colour intensities for each of the RGB colour channels of given pixel. The other pixels in the box are assigned to a vector $ùëì_{i} = ùëü_{i}, ùëî_{i}, ùëè_{i}$ and compared to the centre pixel by calculating a colour distance $\\Delta$. On the proposed approach, ... the Manhattan ($\\Delta_{m}$) ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "agreed-tanzania",
   "metadata": {},
   "source": [
    "$$\n",
    "\\Delta_{m} = \\sum_{k} |f_{i}(k_{i}) - f_{c}(k_{c})|, k \\in r, g, b\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "educational-consumption",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Manhattan(tf.keras.layers.Layer):\n",
    "    def __init__(self):\n",
    "        super(Manhattan, self).__init__()\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        batch_size, patch_number, patch_size, patch_size, channels = tf.unstack(tf.shape(inputs))\n",
    "        outputs = tf.reshape(inputs, shape=(-1, patch_number, patch_size, channels))\n",
    "        \n",
    "        centers = tf.image.resize_with_crop_or_pad(outputs, 1, 1)\n",
    "\n",
    "        outputs = tf.math.subtract(outputs, centers)\n",
    "        outputs = tf.math.abs(outputs)\n",
    "        outputs = tf.math.reduce_sum(outputs, axis=3)\n",
    "        outputs = tf.math.less_equal(outputs, tf.cast(patch_size, dtype=tf.float32))\n",
    "        outputs = tf.cast(outputs, dtype=tf.int32)\n",
    "        outputs = tf.reshape(outputs, shape=(-1, patch_number, patch_size, patch_size))\n",
    "            \n",
    "        \n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ancient-missile",
   "metadata": {},
   "source": [
    "## Calculating probability matrices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "union-trout",
   "metadata": {},
   "source": [
    "According to the article:\n",
    "> After performing this conversion for every box of every given ùêø scale, a structure known as probability matrix is generated. Each element of the matrix corresponds to the probability ùëÉ that ùëö pixels on a scale ùêø are labelled as 1 on each box. ... The matrix is normalized in a way that the sum of the elements in a column is equal to 1, as showed here:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "brilliant-running",
   "metadata": {},
   "source": [
    "$$\n",
    "\\sum_{m=1}^{L^2} P(m, L) = 1, \\forall L\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "plastic-madness",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Probability(tf.keras.layers.Layer):\n",
    "    def __init__(self):\n",
    "        super(Probability, self).__init__()\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        batch_size, patch_number, patch_size, patch_size = tf.unstack(tf.shape(inputs))\n",
    "        \n",
    "        outputs = tf.math.reduce_sum(inputs, axis=(2, 3))\n",
    "        outputs = tf.vectorized_map(lambda image: tf.math.bincount(image, minlength=patch_size ** 2 + 1), outputs)\n",
    "        outputs = tf.math.divide(outputs, patch_number)        \n",
    "        \n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sitting-posting",
   "metadata": {},
   "source": [
    "## Calculating fractal dimensions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "looking-search",
   "metadata": {},
   "source": [
    "According to the article:\n",
    "> FD is the most common technique to evaluate the fractal properties of an image. This is a measure for evaluating the irregularity and the complexity of a fractal. To obtain local FD features from the probability\n",
    "matrix, for each value of ùêø, the FD denominated ùê∑(ùêø) is calculated according to"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fewer-madison",
   "metadata": {},
   "source": [
    "$$\n",
    "D(L) = \\sum_{m=1}^{L^2} \\frac{P(m, L)}{m}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "three-hormone",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FractalDimension(tf.keras.layers.Layer):\n",
    "    def __init__(self):\n",
    "        super(FractalDimension, self).__init__()\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        batch_size, _len = tf.unstack(tf.shape(inputs))\n",
    "        numbers = tf.reshape(\n",
    "            tf.concat(\n",
    "                [tf.constant([1], dtype=tf.float32), tf.range(1, _len, dtype=tf.float32)], \n",
    "                axis=0\n",
    "            ), \n",
    "            shape=(1, -1)\n",
    "        )\n",
    "        \n",
    "        outputs = tf.math.divide(inputs, numbers)\n",
    "        outputs = tf.math.reduce_sum(outputs, axis=1)\n",
    "        \n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ethical-puppy",
   "metadata": {},
   "source": [
    "## Calculating lacunarity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "complimentary-wellington",
   "metadata": {},
   "source": [
    "According to the article:\n",
    "> LAC is a measure complementary to FD and allows to evaluate how the space of a fractal is filled (Ivanovici & Richard, 2009). From the probability matrix, first and second-order moments are calculated with"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "quality-manual",
   "metadata": {},
   "source": [
    "$$\n",
    "\\mu(L) = \\sum_{m=1}^{L^2} mP(m, L)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "romance-speaking",
   "metadata": {},
   "source": [
    "$$\n",
    "\\mu^2(L) = \\sum_{m=1}^{L^2} m^{2}P(m, L)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "standard-maker",
   "metadata": {},
   "source": [
    "> The LAC value for a scale $L$ is given by $\\Lambda$(ùêø), which is obtained according to"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beginning-david",
   "metadata": {},
   "source": [
    "$$\n",
    "\\Lambda(L) = \\frac{\\mu^{2}(L) - (\\mu(L))^{2}}{(\\mu(L))^{2}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "purple-edwards",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Lacunarity(tf.keras.layers.Layer):\n",
    "    def __init__(self):\n",
    "        super(Lacunarity, self).__init__()\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        batch_size, _len = tf.unstack(tf.shape(inputs))\n",
    "        numbers = tf.reshape(\n",
    "            tf.concat(\n",
    "                [tf.constant([1], dtype=tf.float32), tf.range(1, _len, dtype=tf.float32)], \n",
    "                axis=0\n",
    "            ), \n",
    "            shape=(1, -1)\n",
    "        )\n",
    "                \n",
    "        mu_first_2 = tf.math.multiply(inputs, numbers)\n",
    "        mu_first_2 = tf.math.reduce_sum(mu_first_2, axis=1)\n",
    "        mu_first_2 = tf.math.pow(mu_first_2, 2)\n",
    "\n",
    "        mu_second = tf.math.pow(numbers, 2)\n",
    "        mu_second = tf.math.multiply(inputs, mu_second)\n",
    "        mu_second = tf.math.reduce_sum(mu_second, axis=1)\n",
    "\n",
    "        outputs = tf.math.divide(\n",
    "            tf.math.subtract(mu_second, mu_first_2),\n",
    "            mu_first_2\n",
    "        )\n",
    "        \n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "useful-cyprus",
   "metadata": {},
   "source": [
    "## Calculating percolation Q - the average occurrence of percolation on a scale L"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "peaceful-prague",
   "metadata": {},
   "source": [
    "According to the article:\n",
    "> We can also verify whether a box $\\beta_{i}$ is percolating. This can be achieved due to a property that states a percolation threshold for different types of structures. In squared matrices (digital images), this threshold has the value of $p = 0.59275$, which means that if the ratio between pixels labelled as 1 and pixels labelled as 0 is greater or equal than $p$, the matrix is considered as percolating. Let $\\Omega_{i}$ be the number of pixels labelled as 1 in a box $\\beta_{i}$ with size $L \\times L $ , we determine whether such box is percolating according to"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exposed-horizontal",
   "metadata": {},
   "source": [
    "$$\n",
    "q_{i} = \n",
    "\\begin{cases}\n",
    "1, & \\frac{\\Omega_{i}}{L^2} \\ge 0.59275 \\\\\n",
    "0, & \\frac{\\Omega_{i}}{L^2} < 0.59275\n",
    "\\end{cases}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "entitled-intranet",
   "metadata": {},
   "source": [
    "> This results in a binary value for $q_{i}$, wherein 1 indicates that thebox is percolating. The feature $Q(L)$ regards the average occurrence of percolation on a scale $L$ and can be obtained by"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "finished-wrist",
   "metadata": {},
   "source": [
    "$$\n",
    "Q(L) = \\frac{\\sum_{i=1}^{T(L)} q_{i}}{T(L)}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "computational-third",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PercolationQ(tf.keras.layers.Layer):\n",
    "    def __init__(self, threshold=0.59275):\n",
    "        super(PercolationQ, self).__init__()\n",
    "        \n",
    "        self.threshold = threshold\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        batch_size, patch_number, patch_size, patch_size = tf.unstack(tf.shape(inputs))\n",
    "        \n",
    "        outputs = tf.math.reduce_sum(inputs, axis=(2, 3))\n",
    "        outputs = tf.math.divide(outputs, patch_size ** 2)\n",
    "        outputs = tf.math.greater_equal(outputs, self.threshold)\n",
    "        outputs = tf.cast(outputs, dtype=tf.float32)\n",
    "        outputs = tf.math.reduce_mean(outputs, axis=1)\n",
    "        \n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sound-keeping",
   "metadata": {},
   "source": [
    "## Clustering values in binarized patches "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exterior-specific",
   "metadata": {},
   "source": [
    "The next two layers, which calculate percolation C and M, work with value clusters. We clustorize values in a separate layer to speed up calculations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "center-factory",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Clusterize(tf.keras.layers.Layer):\n",
    "    def __init__(self):\n",
    "        super(Clusterize, self).__init__()\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        batch_size, patch_number, patch_size, patch_size = tf.unstack(tf.shape(inputs))\n",
    "        \n",
    "        outputs = tf.reshape(inputs, shape=(-1, patch_size, patch_size))\n",
    "        outputs = tfa.image.connected_components(outputs)\n",
    "        outputs = tf.reshape(outputs, shape=(-1, patch_number, patch_size, patch_size))\n",
    "        \n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "academic-referral",
   "metadata": {},
   "source": [
    "## Calculating percolation C - the average number of clusters per box on a scale L"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bibliographic-remove",
   "metadata": {},
   "source": [
    "According to the article:\n",
    "> Let $c_{i}$ be the number of clusters on a box $\\beta_{i}$, the feature $C(L)$ that represents the average number of clusters per box on a scale $L$ is given by"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "taken-somerset",
   "metadata": {},
   "source": [
    "$$\n",
    "C(L) = \\frac{\\sum_{i=1}^{T(L)} c_{i}}{T(L)}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "embedded-roman",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PercolationC(tf.keras.layers.Layer):\n",
    "    def __init__(self):\n",
    "        super(PercolationC, self).__init__()\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        outputs = tf.cast(inputs, dtype=tf.float32)\n",
    "        outputs = tf.math.reduce_max(outputs, axis=(2, 3))\n",
    "        outputs = tf.math.reduce_mean(outputs, axis=1)\n",
    "        \n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "narrow-sweden",
   "metadata": {},
   "source": [
    "## Calculating percolation M - the average coverage area of the largest cluster on a scale L"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "front-warehouse",
   "metadata": {},
   "source": [
    "According to the article:\n",
    ">Another feature that can be obtained is the average coverage area of the largest cluster in a box and is given by $M(L)$. Let $m_{i}$ be the size in pixels of the largest cluster of the box $\\beta_{i}$. The feature $M(L)$ is givenaccording to"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "solid-alfred",
   "metadata": {},
   "source": [
    "$$\n",
    "M(L) = \\frac{\\sum_{i=1}^{T(L)} \\frac{m_{i}}{L^2}}{T(L)}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "macro-certificate",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PercolationM(tf.keras.layers.Layer):\n",
    "    def __init__(self):\n",
    "        super(PercolationM, self).__init__()\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        batch_size, patch_number, patch_size, patch_size = tf.unstack(tf.shape(inputs))\n",
    "        \n",
    "        outputs = tf.reshape(inputs, shape=(-1, patch_number, patch_size ** 2))\n",
    "        outputs = tf.map_fn(lambda image: tf.math.reduce_max(tf.math.bincount(image)), outputs)\n",
    "        outputs = tf.cast(outputs, dtype=tf.float32)\n",
    "        \n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nervous-bradley",
   "metadata": {},
   "source": [
    "## Assembling fractal features into an image channel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "invisible-front",
   "metadata": {},
   "source": [
    "According to the article:\n",
    "> To serve as input for the incoming CNN classification, the feature vectors generated on the previous layers of the network must be converted into feature matrices. To do so, the 100 features obtained by each distance $\\Delta$ are rearranged as a $10 \\times 10 \\times 10$ matrix. The matrices generated by $\\Delta_{h}$, $\\Delta_{e}$ and $\\Delta_{m}$ correspond to the R, G and B colour channels, respectively. ... Since each of the functions $C(L), Q(L), M(L), \\Lambda(L)$ and $D(L)$, obtained from a specific $\\Delta$, generate 20 features, each function is fit exactly into 2 columns of the matrix.\n",
    "\n",
    ">Since each of the functions $C(L), Q(L), M(L), \\Lambda(L)$ and $D(L)$, obtained from a specific $\\Delta$, generate 20 features, each function is fit exactly into 2 columns of the matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "rubber-bobby",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AssembleChannel(tf.keras.layers.Layer):\n",
    "    def __init__(self):\n",
    "        super(AssembleChannel, self).__init__()\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        fractal_dimension = tf.convert_to_tensor(inputs[0])\n",
    "        fractal_dimension = tf.transpose(fractal_dimension, perm=(1, 0))\n",
    "        \n",
    "        lacunarity = tf.convert_to_tensor(inputs[1])\n",
    "        lacunarity = tf.transpose(lacunarity, perm=(1, 0))\n",
    "        \n",
    "        percolation_q = tf.convert_to_tensor(inputs[2])\n",
    "        percolation_q = tf.transpose(percolation_q, perm=(1, 0))\n",
    "        \n",
    "        percolation_c = tf.convert_to_tensor(inputs[3])\n",
    "        percolation_c = tf.transpose(percolation_c, perm=(1, 0))\n",
    "        \n",
    "        percolation_m = tf.convert_to_tensor(inputs[4])\n",
    "        percolation_m = tf.transpose(percolation_m, perm=(1, 0))\n",
    "        \n",
    "        outputs = tf.concat([\n",
    "            percolation_c,\n",
    "            percolation_q,\n",
    "            percolation_m,\n",
    "            lacunarity,\n",
    "            fractal_dimension\n",
    "        ], axis=1)\n",
    "        outputs = tf.reshape(outputs, shape=(-1, 10, 10))\n",
    "        \n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "answering-knitting",
   "metadata": {},
   "source": [
    "## Organising fractal feature extraction into layers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "compliant-romania",
   "metadata": {},
   "source": [
    "We move feature extraction to layers to simplify and clarify the code."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "novel-camcorder",
   "metadata": {},
   "source": [
    "### based on Chebyshev distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "illegal-therapist",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChebyshevFeatures(tf.keras.layers.Layer):\n",
    "    def __init__(self):\n",
    "        super(ChebyshevFeatures, self).__init__()\n",
    "        \n",
    "        self.chebyshev = Chebyshev()\n",
    "        self.probability = Probability()\n",
    "        self.clusterize = Clusterize()\n",
    "        \n",
    "        self.fractal_dimension = FractalDimension()\n",
    "        self.lacunarity = Lacunarity()\n",
    "        self.percolation_q = PercolationQ()\n",
    "        self.percolation_c = PercolationC()\n",
    "        self.percolation_m = PercolationM()\n",
    "        \n",
    "        self.assemble_channel = AssembleChannel()\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        chebyshevs = [self.chebyshev(i) for i in inputs]\n",
    "        \n",
    "        probability = [self.probability(ch) for ch in chebyshevs]\n",
    "        cluster = [self.clusterize(ch) for ch in chebyshevs]\n",
    "        \n",
    "        fractal_dimension = [self.fractal_dimension(ch) for ch in probability]\n",
    "        lacunarity = [self.lacunarity(ch) for ch in probability]        \n",
    "        percolation_q = [self.percolation_q(ch) for ch in chebyshevs]\n",
    "        percolation_c = [self.percolation_c(ch) for ch in cluster]\n",
    "        percolation_m = [self.percolation_m(ch) for ch in cluster]\n",
    "        \n",
    "        features = self.assemble_channel([\n",
    "            fractal_dimension,\n",
    "            lacunarity,\n",
    "            percolation_q,\n",
    "            percolation_c,\n",
    "            percolation_m\n",
    "        ])\n",
    "        \n",
    "        return features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "psychological-israel",
   "metadata": {},
   "source": [
    "### based on Euclidean distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "tight-jacksonville",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EuclideanFeatures(tf.keras.layers.Layer):\n",
    "    def __init__(self):\n",
    "        super(EuclideanFeatures, self).__init__()\n",
    "        \n",
    "        self.euclidean = Euclidean()\n",
    "        self.probability = Probability()\n",
    "        self.clusterize = Clusterize()\n",
    "        \n",
    "        self.fractal_dimension = FractalDimension()\n",
    "        self.lacunarity = Lacunarity()\n",
    "        self.percolation_q = PercolationQ()\n",
    "        self.percolation_c = PercolationC()\n",
    "        self.percolation_m = PercolationM()\n",
    "        \n",
    "        self.assemble_channel = AssembleChannel()\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        euclideans = [self.euclidean(i) for i in inputs]\n",
    "        \n",
    "        probability = [self.probability(eu) for eu in euclideans]\n",
    "        cluster = [self.clusterize(eu) for eu in euclideans]\n",
    "        \n",
    "        fractal_dimension = [self.fractal_dimension(eu) for eu in probability]\n",
    "        lacunarity = [self.lacunarity(eu) for eu in probability]        \n",
    "        percolation_q = [self.percolation_q(eu) for eu in euclideans]\n",
    "        percolation_c = [self.percolation_c(eu) for eu in cluster]\n",
    "        percolation_m = [self.percolation_m(eu) for eu in cluster]\n",
    "        \n",
    "        features = self.assemble_channel([\n",
    "            fractal_dimension,\n",
    "            lacunarity,\n",
    "            percolation_q,\n",
    "            percolation_c,\n",
    "            percolation_m\n",
    "        ])\n",
    "        \n",
    "        return features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "relevant-consumer",
   "metadata": {},
   "source": [
    "### based on Manhattan distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "academic-catalyst",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ManhattanFeatures(tf.keras.layers.Layer):\n",
    "    def __init__(self):\n",
    "        super(ManhattanFeatures, self).__init__()\n",
    "        \n",
    "        self.manhattan = Manhattan()\n",
    "        self.probability = Probability()\n",
    "        self.clusterize = Clusterize()\n",
    "        \n",
    "        self.fractal_dimension = FractalDimension()\n",
    "        self.lacunarity = Lacunarity()\n",
    "        self.percolation_q = PercolationQ()\n",
    "        self.percolation_c = PercolationC()\n",
    "        self.percolation_m = PercolationM()\n",
    "        \n",
    "        self.assemble_channel = AssembleChannel()\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        manhattans = [self.manhattan(i) for i in inputs]\n",
    "        \n",
    "        probability = [self.probability(mh) for mh in manhattans]\n",
    "        cluster = [self.clusterize(mh) for mh in manhattans]\n",
    "        \n",
    "        fractal_dimension = [self.fractal_dimension(mh) for mh in probability]\n",
    "        lacunarity = [self.lacunarity(mh) for mh in probability]        \n",
    "        percolation_q = [self.percolation_q(mh) for mh in manhattans]\n",
    "        percolation_c = [self.percolation_c(mh) for mh in cluster]\n",
    "        percolation_m = [self.percolation_m(mh) for mh in cluster]\n",
    "        \n",
    "        features = self.assemble_channel([\n",
    "            fractal_dimension,\n",
    "            lacunarity,\n",
    "            percolation_q,\n",
    "            percolation_c,\n",
    "            percolation_m\n",
    "        ])\n",
    "        \n",
    "        return features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pleased-debut",
   "metadata": {},
   "source": [
    "## Assembling fractal features into images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "psychological-termination",
   "metadata": {},
   "source": [
    "We assemble fractal features into images, such that each set of fractal features corresponds to a colour channel (R, G, B)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "mediterranean-sympathy",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AssembleImage(tf.keras.layers.Layer):\n",
    "    def __init__(self):\n",
    "        super(AssembleImage, self).__init__()\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        outputs = tf.stack(inputs)\n",
    "        outputs = tf.transpose(outputs, perm=(1, 2, 3, 0))\n",
    "        \n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "going-expert",
   "metadata": {},
   "source": [
    "## Organising the fractal feature extraction layers into the single, fractal image layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "announced-refund",
   "metadata": {},
   "source": [
    "To further simplify the code, we will gather the fractal feature extraction into the single layer, which generates artificial fractal image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "stable-belize",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FractalImage(tf.keras.layers.Layer):\n",
    "    def __init__(self):\n",
    "        super(FractalImage, self).__init__()\n",
    "        \n",
    "        self.patchifies = [Patchify(patch_size) for patch_size in range(3, 41 + 1, 2)]\n",
    "        \n",
    "        self.chebyshev_features = ChebyshevFeatures()\n",
    "        self.euclidean_features = EuclideanFeatures()\n",
    "        self.manhattan_features = ManhattanFeatures()\n",
    "        \n",
    "        self.assemble_image = AssembleImage()\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        patchifies = [patchify(inputs) for patchify in self.patchifies]\n",
    "        \n",
    "        chebyshev_features = self.chebyshev_features(patchifies)\n",
    "        euclidean_features = self.euclidean_features(patchifies)\n",
    "        manhattan_features = self.manhattan_features(patchifies)\n",
    "        \n",
    "        outputs = self.assemble_image([\n",
    "            chebyshev_features,\n",
    "            euclidean_features,\n",
    "            manhattan_features\n",
    "        ])\n",
    "        \n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "improving-correspondence",
   "metadata": {},
   "source": [
    "## Assembling the fractal neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "existing-excess",
   "metadata": {},
   "source": [
    "So, here we are assembling the fractal neural network from the pieces mentioned above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "suspended-state",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FractalNeuralNetwork(tf.keras.Model):\n",
    "    TARGET_WIDTH = 224\n",
    "    TARGET_HEIGHT = 224\n",
    "    \n",
    "    def __init__(self, class_number):\n",
    "        super(FractalNeuralNetwork, self).__init__()\n",
    "        \n",
    "        self.fractal_image = FractalImage()\n",
    "        self.resize = tf.keras.layers.Resizing(width=self.TARGET_WIDTH, height=self.TARGET_HEIGHT)\n",
    "        self.rescale_original = tf.keras.layers.Rescaling(scale=1./255)\n",
    "        self.rescale_fractal = tf.keras.layers.Lambda(lambda x: tf.math.divide(x, tf.math.reduce_max(x)))\n",
    "        self.mobilenet_v2 = hub.KerasLayer(\n",
    "            \"https://tfhub.dev/google/tf2-preview/mobilenet_v2/feature_vector/4\", \n",
    "            output_shape=[1280], \n",
    "            trainable=False\n",
    "        )\n",
    "        self.combine = tf.keras.layers.Add()\n",
    "        self.score = tf.keras.layers.Dense(class_number, activation='softmax')\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        fractal_outputs = self.fractal_image(inputs)\n",
    "        fractal_outputs = self.resize(fractal_outputs)\n",
    "        fractal_outputs = self.rescale_fractal(fractal_outputs)\n",
    "        fractal_outputs = self.mobilenet_v2(fractal_outputs)\n",
    "        \n",
    "        original_outputs = self.rescale_original(inputs)\n",
    "        original_outputs = self.mobilenet_v2(original_outputs)\n",
    "        \n",
    "        outputs = self.combine([fractal_outputs, original_outputs])\n",
    "        outputs = self.score(outputs)\n",
    "        \n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "latin-friday",
   "metadata": {},
   "source": [
    "# Data loading"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "judicial-mounting",
   "metadata": {},
   "source": [
    "## Data source"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "loved-parent",
   "metadata": {},
   "source": [
    "As a data source, we use the ISIC Archive."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "detailed-prague",
   "metadata": {},
   "source": [
    "The ISIC Archive is an open source platform with publicly available images of skin lesions under Creative Commons licenses. The images are associated with ground-truth diagnoses and other clinical metadata. Images can be queried using faceted search and downloaded individually or in batches. The initial focus of the archive has been on dermoscopy images of individual skin lesions, as these images are inherently standardized by the use of a specialized acquisition device and devoid of many of the privacy challenges associated with clinical images. To date, the images have been provided by specialized melanoma centers from around the world. The archive is designed to accept contributions from new sources under the Terms of Use and welcomes new contributors. There are ongoing efforts to supplement the dermoscopy images in the archive with close-up clinical images and a broader representation of skin types. The images in the Archive are used to support educational efforts through linkage with Dermoscopedia and are used for Grand Challenges and Live Challenges to engage the computer science community for the development of diagnostic AI."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "expected-baptist",
   "metadata": {},
   "source": [
    "For more information, go to [ISIC Archive web site](https://www.isic-archive.com/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "hollow-links",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 798 images belonging to 4 classes.\n",
      "Found 197 images belonging to 4 classes.\n"
     ]
    }
   ],
   "source": [
    "generator = tf.keras.preprocessing.image.ImageDataGenerator(\n",
    "    rotation_range=180,\n",
    "    horizontal_flip=True,\n",
    "    vertical_flip=True,\n",
    "    brightness_range=(0.2, 1.5),\n",
    "    validation_split=0.2,\n",
    ")\n",
    "\n",
    "\n",
    "training_set = generator.flow_from_directory(\n",
    "    '../../data1000', \n",
    "    target_size=(224, 224), \n",
    "    batch_size=128, \n",
    "    class_mode='categorical', \n",
    "    subset='training'\n",
    ")\n",
    "validation_set = generator.flow_from_directory(\n",
    "    '../../data1000', \n",
    "    target_size=(224, 224), \n",
    "    batch_size=128, \n",
    "    class_mode='categorical', \n",
    "    subset='validation'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "hollywood-invention",
   "metadata": {},
   "outputs": [],
   "source": [
    "CLASS_NUMBER = len(training_set.class_indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lined-synthetic",
   "metadata": {},
   "source": [
    "# Model training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "institutional-circus",
   "metadata": {},
   "source": [
    "## Preparing TensorFlow callbacks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "applied-insulin",
   "metadata": {},
   "source": [
    "For our convenience, we create a few TensorFlow callbacks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "inappropriate-symposium",
   "metadata": {},
   "source": [
    "### The TensorBoard callback"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "negative-depth",
   "metadata": {},
   "source": [
    "We want to see how the training is going. We add the callback, which will log the metrics to TensorBoard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "pediatric-guess",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dir = '../logs/fit/' + datetime.datetime.now().strftime('fractalnet')\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "interesting-bangkok",
   "metadata": {},
   "source": [
    "### The EarlyStopping callback"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "found-cambodia",
   "metadata": {},
   "source": [
    "This callback stops training when the metrics (e.g. validation loss) are not improving,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "recreational-brown",
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stop_callback = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor=\"val_loss\", \n",
    "    min_delta=0.01, \n",
    "    patience=3, \n",
    "    restore_best_weights=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "otherwise-massage",
   "metadata": {},
   "source": [
    "### The ModelCheckpoint callback"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "linear-stevens",
   "metadata": {},
   "source": [
    "This callback saves the model with the best metrics during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "adopted-proposal",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path = 'checkpoints/fractalnet.ckpt'\n",
    "\n",
    "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    checkpoint_path,\n",
    "    monitor='val_loss',\n",
    "    verbose=1,\n",
    "    save_best_only=True,\n",
    "    save_weights_only=False,\n",
    "    save_freq='epoch',\n",
    "    mode='auto'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "knowing-manner",
   "metadata": {},
   "source": [
    "## Actual training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "victorian-proceeding",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.4941 - accuracy: 0.8371 \n",
      "Epoch 1: val_loss improved from inf to 0.21772, saving model to checkpoints/fractalnet.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as chebyshev_features_layer_call_fn, chebyshev_features_layer_call_and_return_conditional_losses, euclidean_features_layer_call_fn, euclidean_features_layer_call_and_return_conditional_losses, manhattan_features_layer_call_fn while saving (showing 5 of 102). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7/7 [==============================] - 407s 53s/step - loss: 0.4941 - accuracy: 0.8371 - val_loss: 0.2177 - val_accuracy: 0.9645\n",
      "Epoch 2/10\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.2540 - accuracy: 0.9574 \n",
      "Epoch 2: val_loss improved from 0.21772 to 0.16671, saving model to checkpoints/fractalnet.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as chebyshev_features_layer_call_fn, chebyshev_features_layer_call_and_return_conditional_losses, euclidean_features_layer_call_fn, euclidean_features_layer_call_and_return_conditional_losses, manhattan_features_layer_call_fn while saving (showing 5 of 102). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7/7 [==============================] - 293s 45s/step - loss: 0.2540 - accuracy: 0.9574 - val_loss: 0.1667 - val_accuracy: 0.9492\n",
      "Epoch 3/10\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.2032 - accuracy: 0.9624 \n",
      "Epoch 3: val_loss improved from 0.16671 to 0.13145, saving model to checkpoints/fractalnet.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as chebyshev_features_layer_call_fn, chebyshev_features_layer_call_and_return_conditional_losses, euclidean_features_layer_call_fn, euclidean_features_layer_call_and_return_conditional_losses, manhattan_features_layer_call_fn while saving (showing 5 of 102). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7/7 [==============================] - 294s 45s/step - loss: 0.2032 - accuracy: 0.9624 - val_loss: 0.1314 - val_accuracy: 0.9695\n",
      "Epoch 4/10\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1800 - accuracy: 0.9674 \n",
      "Epoch 4: val_loss improved from 0.13145 to 0.12983, saving model to checkpoints/fractalnet.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as chebyshev_features_layer_call_fn, chebyshev_features_layer_call_and_return_conditional_losses, euclidean_features_layer_call_fn, euclidean_features_layer_call_and_return_conditional_losses, manhattan_features_layer_call_fn while saving (showing 5 of 102). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7/7 [==============================] - 297s 46s/step - loss: 0.1800 - accuracy: 0.9674 - val_loss: 0.1298 - val_accuracy: 0.9594\n",
      "Epoch 5/10\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1609 - accuracy: 0.9762 \n",
      "Epoch 5: val_loss did not improve from 0.12983\n",
      "7/7 [==============================] - 133s 19s/step - loss: 0.1609 - accuracy: 0.9762 - val_loss: 0.1685 - val_accuracy: 0.9543\n",
      "Epoch 6/10\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1325 - accuracy: 0.9837 \n",
      "Epoch 6: val_loss did not improve from 0.12983\n",
      "7/7 [==============================] - 138s 19s/step - loss: 0.1325 - accuracy: 0.9837 - val_loss: 0.1483 - val_accuracy: 0.9594\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2acd9fe6a2e0>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = FractalNeuralNetwork(class_number=CLASS_NUMBER)\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model.fit(\n",
    "    training_set, \n",
    "    validation_data=validation_set, \n",
    "    epochs=10,\n",
    "    callbacks=[\n",
    "        tensorboard_callback,\n",
    "        early_stop_callback,\n",
    "        checkpoint_callback\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "noticed-liverpool",
   "metadata": {},
   "source": [
    "# Model validation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "large-relative",
   "metadata": {},
   "source": [
    "## Loading the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "greek-processor",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 899 images belonging to 4 classes.\n"
     ]
    }
   ],
   "source": [
    "testing_set = generator.flow_from_directory(\n",
    "    '../../data1000-test',\n",
    "    target_size=(224, 224),\n",
    "    batch_size=32,\n",
    "    class_mode='categorical'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "smoking-treasury",
   "metadata": {},
   "source": [
    "## Making diagnoses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "swedish-bicycle",
   "metadata": {},
   "outputs": [],
   "source": [
    "true_labels = np.concatenate([testing_set[i][1] for i in range(len(testing_set))], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "interim-diagnosis",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_labels = model.predict(testing_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "honey-format",
   "metadata": {},
   "source": [
    "## Plot the ROC Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "adjusted-lingerie",
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr = dict()\n",
    "tpr = dict()\n",
    "auc_metric = dict()\n",
    "\n",
    "diagnosis_index_dict = {v: k for k, v in testing_set.class_indices.items()}\n",
    "\n",
    "for i in range(CLASS_NUMBER):\n",
    "    diagnosis = diagnosis_index_dict[i]\n",
    "    fpr[diagnosis], tpr[diagnosis], _ = roc_curve(true_labels[:, i], predicted_labels[:, i])\n",
    "    auc_metric[diagnosis] = auc(fpr[diagnosis], tpr[diagnosis])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "musical-situation",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline \n",
    "for diagnosis in testing_set.class_indices:\n",
    "    plt.plot(fpr[diagnosis], tpr[diagnosis], label=diagnosis)\n",
    "    \n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver operating characteristic')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "burning-maker",
   "metadata": {},
   "source": [
    "## Show AUC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "cross-hierarchy",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'BCC': 0.7973273942093542,\n",
       " 'BKL': 0.525564803804994,\n",
       " 'MEL': 0.5828364222401289,\n",
       " 'NV': 0.5733704046957059}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "auc_metric"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
