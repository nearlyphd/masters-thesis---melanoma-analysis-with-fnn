{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Melanoma analysis with fractal neural networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook shows how good is [Fractal neural network](#Fractal-neural-network) for [melanoma](#Melanoma) analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Matplotlib created a temporary config/cache directory at /tmp/matplotlib-ysmfyyfn because the default path (/.config/matplotlib) is not a writable directory; it is highly recommended to set the MPLCONFIGDIR environment variable to a writable directory, in particular to speed up the import of Matplotlib and to better support multiprocessing.\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow_addons as tfa\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_curve, auc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check if a GPU is available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.config.list_physical_devices('GPU')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Melanoma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Melanoma__, also redundantly known as __malignant melanoma__, is a type of skin cancer that develops from the pigment-producing cells known as melanocytes. Melanomas typically occur in the skin, but may rarely occur in the mouth, intestines, or eye (uveal melanoma). In women, they most commonly occur on the legs, while in men, they most commonly occur on the back. About 25% of melanomas develop from moles. Changes in a mole that can indicate melanoma include an increase in size, irregular edges, change in color, itchiness, or skin breakdown."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![melanoma image](../assets/melanoma.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center; font-weight: bold\">Pic.1. A melanoma of approximately 2.5 cm (1 in) by 1.5 cm (0.6 in)</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The primary cause of melanoma is ultraviolet light (UV) exposure in those with low levels of the skin pigment melanin. The UV light may be from the sun or other sources, such as tanning devices. Those with many moles, a history of affected family members, and poor immune function are at greater risk. A number of rare genetic conditions, such as xeroderma pigmentosum, also increase the risk. Diagnosis is by biopsy and analysis of any skin lesion that has signs of being potentially cancerous."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Melanoma is the most dangerous type of skin cancer. Globally, in 2012, it newly occurred in 232,000 people. In 2015, 3.1 million people had active disease, which resulted in 59,800 deaths. Australia and New Zealand have the highest rates of melanoma in the world. High rates also occur in Northern Europe and North America, while it is less common in Asia, Africa, and Latin America. In the United States, melanoma occurs about 1.6 times more often in men than women. Melanoma has become more common since the 1960s in areas mostly populated by people of European descent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fractal neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We propose an ensemble model based on handcrafted fractal features and deep learning that consists of combining the classification of two CNNs by applying the sum rule. We apply feature extraction to obtain 300 fractal features from different\n",
    "dermoscopy datasets. These features are reshaped into a 10 × 10 × 3 matrix to compose an artificial image that\n",
    "is given as input to the first CNN. The second CNN model receives as input the correspondent original image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![CNN image](../assets/fnn.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center; font-weight: bold\">Pic.2. Overview of the proposed FNN model.</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to learn more about fractal neural networks, read [here](https://www.sciencedirect.com/science/article/abs/pii/S0957417420308563)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Developing the layer that divides images into patches."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to the acticle:\n",
    "> One of the approaches available in the literature for multiscale\n",
    "analysis is the gliding-box algorithm (Ivanovici & Richard, 2011). The\n",
    "main advantage of this approach is that it can be applied on datasets\n",
    "containing images with different resolutions since the output features\n",
    "are given in relation to the scale instead of being absolute values.\n",
    "This algorithm consists in placing a box 𝛽𝑖\n",
    "sized 𝐿 × 𝐿 on the left\n",
    "superior corner of the image, wherein 𝐿 is given in pixels. This box\n",
    "glides through the image, one column and then one row at a time. After\n",
    "reaching the end of the image, the box is repositioned at the starting\n",
    "point and the value of 𝐿 is increased by 2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The gliding-box method will not be used since it consumes too much RAM. We'll employ a box-counting approach, which basically means we'll partition the images into non-overlapping chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BoxCountingPatch(tf.keras.layers.Layer):\n",
    "    def __init__(self, box_size):\n",
    "        super(BoxCountingPatch, self).__init__()\n",
    "        \n",
    "        self.box_size = box_size\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        patched_inputs = tf.image.extract_patches(\n",
    "            inputs,\n",
    "            sizes=(1, self.box_size, self.box_size, 1),\n",
    "            strides=(1, self.box_size, self.box_size, 1),\n",
    "            rates=(1, 1, 1, 1),\n",
    "            padding='SAME'\n",
    "        )\n",
    "        \n",
    "        patched_inputs_shape = tf.shape(patched_inputs)\n",
    "        _, rows, cols, _ = tf.unstack(patched_inputs_shape)\n",
    "        \n",
    "        patched_inputs = tf.reshape(patched_inputs, shape=(-1, rows * cols, self.box_size, self.box_size, 3))\n",
    "        \n",
    "        return patched_inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Developing the layer that creates an array of binary values from image patches using the Chebyshev colour distance function applied to the patch centre and each pixel."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to the article:\n",
    "> For each time the box 𝛽<sub>𝑖</sub> is moved, a multidimensional analysis of colour similarity is performed for every pixel inside it. This is done by assigning the centre pixel to a vector 𝑓<sub>𝑐</sub> = 𝑟<sub>𝑐</sub>, 𝑔<sub>𝑐</sub>, 𝑏<sub>𝑐</sub>, where 𝑟<sub>𝑐</sub>, 𝑔<sub>𝑐</sub> and 𝑏<sub>𝑐</sub> correspond to the colour intensities for each of the RGB colour channels of given pixel. The other pixels in the box are assigned to a vector 𝑓<sub>𝑖</sub> = 𝑟<sub>𝑖</sub>, 𝑔<sub>𝑖</sub>, 𝑏<sub>𝑖</sub> and compared to the centre pixel by calculating a colour distance 𝛥. On the proposed approach, the Chebyshev (𝛥<sub>ℎ</sub>) ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following equation is used to compute the Chebyshev distance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\Delta_{h} = max(|f_{i}(k_{i}) - f_{c}(k_{c})|), k \\in r, g, b. \n",
    "$$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChebyshevBinaryPatch(tf.keras.layers.Layer):\n",
    "    def __init__(self):\n",
    "        super(ChebyshevBinaryPatch, self).__init__()\n",
    "\n",
    "    def call(self, inputs):\n",
    "        def helper(_input_):\n",
    "            _input__shape = tf.shape(_input_)\n",
    "            _, number_of_patches, box_size, _, channels = tf.unstack(_input__shape)\n",
    "            _input_ = tf.reshape(_input_, shape=(-1, box_size, box_size, channels))\n",
    "            \n",
    "            centers = tf.image.resize_with_crop_or_pad(_input_, 1, 1)\n",
    "            \n",
    "            binary = tf.math.subtract(_input_, centers)\n",
    "            binary = tf.math.abs(binary)\n",
    "            binary = tf.math.reduce_max(binary, axis=3)\n",
    "            binary = tf.math.less_equal(binary, tf.cast(box_size, dtype=tf.float32))\n",
    "            binary = tf.cast(binary, dtype=tf.int32)\n",
    "            binary = tf.reshape(binary, shape=(-1, number_of_patches, box_size, box_size))\n",
    "            \n",
    "            return binary\n",
    "        \n",
    "        return [helper(_input_) for _input_ in inputs]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Developing the layer that creates an array of binary values from image patches using the Euclidean colour distance function applied to the patch centre and each pixel."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to the article:\n",
    "> For each time the box 𝛽<sub>𝑖</sub> is moved, a multidimensional analysis of colour similarity is performed for every pixel inside it. This is done by assigning the centre pixel to a vector 𝑓<sub>𝑐</sub> = 𝑟<sub>𝑐</sub>, 𝑔<sub>𝑐</sub>, 𝑏<sub>𝑐</sub>, where 𝑟<sub>𝑐</sub>, 𝑔<sub>𝑐</sub> and 𝑏<sub>𝑐</sub> correspond to the colour intensities for each of the RGB colour channels of given pixel. The other pixels in the box are assigned to a vector 𝑓<sub>𝑖</sub> = 𝑟<sub>𝑖</sub>, 𝑔<sub>𝑖</sub>, 𝑏<sub>𝑖</sub> and compared to the centre pixel by calculating a colour distance 𝛥. On the proposed approach, ... Euclidean (𝛥<sub>e</sub>) .."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\Delta_{e} = \\sqrt{\\sum_{k} (f_{i}(k_{i}) - f_{c}(k_{c}))^2}, k \\in r, g, b\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EuclideanBinaryPatch(tf.keras.layers.Layer):\n",
    "    def __init__(self):\n",
    "        super(EuclideanBinaryPatch, self).__init__()\n",
    "\n",
    "    def call(self, inputs):\n",
    "        def helper(_input_):\n",
    "            _input__shape = tf.shape(_input_)\n",
    "            _, number_of_patches, box_size, _, channels = tf.unstack(_input__shape)\n",
    "            _input_ = tf.reshape(_input_, shape=(-1, box_size, box_size, channels))\n",
    "            \n",
    "            centers = tf.image.resize_with_crop_or_pad(_input_, 1, 1)\n",
    "            \n",
    "            binary = tf.math.subtract(_input_, centers)\n",
    "            binary = tf.math.pow(_input_, 2)\n",
    "            binary = tf.math.reduce_sum(binary, axis=3)\n",
    "            binary = tf.math.pow(binary, 0.5)\n",
    "            binary = tf.math.less_equal(binary, tf.cast(box_size, dtype=tf.float32))\n",
    "            binary = tf.cast(binary, dtype=tf.int32)\n",
    "            binary = tf.reshape(binary, shape=(-1, number_of_patches, box_size, box_size))\n",
    "            \n",
    "            return binary\n",
    "        \n",
    "        return [helper(_input_) for _input_ in inputs]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Developing the layer that creates an array of binary values from image patches using the Manhattan colour distance function applied to the patch centre and each pixel."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to the article:\n",
    "> For each time the box 𝛽<sub>𝑖</sub> is moved, a multidimensional analysis of colour similarity is performed for every pixel inside it. This is done by assigning the centre pixel to a vector 𝑓<sub>𝑐</sub> = 𝑟<sub>𝑐</sub>, 𝑔<sub>𝑐</sub>, 𝑏<sub>𝑐</sub>, where 𝑟<sub>𝑐</sub>, 𝑔<sub>𝑐</sub> and 𝑏<sub>𝑐</sub> correspond to the colour intensities for each of the RGB colour channels of given pixel. The other pixels in the box are assigned to a vector 𝑓<sub>𝑖</sub> = 𝑟<sub>𝑖</sub>, 𝑔<sub>𝑖</sub>, 𝑏<sub>𝑖</sub> and compared to the centre pixel by calculating a colour distance 𝛥. On the proposed approach, ... Manhattan (𝛥<sub>m</sub>) .. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\Delta_{m} = \\sum_{k} |f_{i}(k_{i}) - f_{c}(k_{c})|, k \\in r, g, b\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ManhattanBinaryPatch(tf.keras.layers.Layer):\n",
    "    def __init__(self):\n",
    "        super(ManhattanBinaryPatch, self).__init__()\n",
    "\n",
    "    def call(self, inputs):\n",
    "        def helper(_input_):\n",
    "            _input__shape = tf.shape(_input_)\n",
    "            _, number_of_patches, box_size, _, channels = tf.unstack(_input__shape)\n",
    "            _input_ = tf.reshape(_input_, shape=(-1, box_size, box_size, channels))\n",
    "            \n",
    "            centers = tf.image.resize_with_crop_or_pad(_input_, 1, 1)\n",
    "            \n",
    "            binary = tf.math.subtract(_input_, centers)\n",
    "            binary = tf.math.abs(binary)\n",
    "            binary = tf.math.reduce_sum(binary, axis=3)\n",
    "            binary = tf.math.less_equal(binary, tf.cast(box_size, dtype=tf.float32))\n",
    "            binary = tf.cast(binary, dtype=tf.int32)\n",
    "            binary = tf.reshape(binary, shape=(-1, number_of_patches, box_size, box_size))\n",
    "            \n",
    "            return binary\n",
    "        \n",
    "        return [helper(_input_) for _input_ in inputs]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Developing the layer that calculates probability matrices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to the article:\n",
    "> After performing this conversion for every box of every given 𝐿 scale, a structure known as probability matrix is generated. Each element of the matrix corresponds to the probability 𝑃 that 𝑚 pixels on a scale 𝐿 are labelled as 1 on each box. ... The matrix is normalized in a way that the sum of the elements in a column is equal to 1, as showed here:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\sum_{m=1}^{L^2} P(m, L) = 1, \\forall L\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProbabilityMatrix(tf.keras.layers.Layer):\n",
    "    def __init__(self):\n",
    "        super(ProbabilityMatrix, self).__init__()\n",
    "\n",
    "    def call(self, inputs):\n",
    "        color_distance_inputs = []\n",
    "        \n",
    "        for color_distance_input in inputs:\n",
    "            box_outputs = []\n",
    "            \n",
    "            for box_input in color_distance_input:\n",
    "                number_of_ones_for_every_patch = tf.map_fn(\n",
    "                    lambda batch: tf.map_fn(\n",
    "                        lambda patch: tf.math.reduce_sum(patch),\n",
    "                        batch\n",
    "                    ),\n",
    "                    box_input\n",
    "                )\n",
    "                \n",
    "                box_input_shape = tf.shape(box_input)\n",
    "                _, number_of_patches, box_size, _ = tf.unstack(box_input_shape)\n",
    "                \n",
    "                probabilities = tf.math.bincount(\n",
    "                    number_of_ones_for_every_patch,\n",
    "                    minlength=1, \n",
    "                    maxlength=box_size ** 2, \n",
    "                    axis=-1\n",
    "                )\n",
    "                probabilities = tf.math.divide(probabilities, number_of_patches)\n",
    "                \n",
    "                probabilities = tf.map_fn(\n",
    "                    lambda x: x[0] / x[1], \n",
    "                    elems=(probabilities, tf.math.reduce_sum(probabilities, axis=1)),\n",
    "                    fn_output_signature=tf.float64\n",
    "                )\n",
    "                \n",
    "                \n",
    "                box_outputs.append(probabilities)\n",
    "                \n",
    "            color_distance_inputs.append(box_outputs)\n",
    "            \n",
    "        return color_distance_inputs\n",
    "                \n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Developing the layer that calculates fractal dimensions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to the article:\n",
    "> FD is the most common technique to evaluate the fractal properties of an image. This is a measure for evaluating the irregularity and the complexity of a fractal. To obtain local FD features from the probability\n",
    "matrix, for each value of 𝐿, the FD denominated 𝐷(𝐿) is calculated according to"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "D(L) = \\sum_{m=1}^{L^2} \\frac{P(m, L)}{m}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FractalDimension(tf.keras.layers.Layer):\n",
    "    def __init__(self):\n",
    "        super(FractalDimension, self).__init__()\n",
    "\n",
    "    def call(self, inputs):\n",
    "        color_distance_outputs = []\n",
    "        \n",
    "        for color_distance_input in inputs:\n",
    "            box_outputs = []\n",
    "            \n",
    "            for box_input in color_distance_input:\n",
    "                box_input_shape = tf.shape(box_input)\n",
    "                _, box_input_len = tf.unstack(box_input_shape)\n",
    "                \n",
    "                probability_numbers = tf.range(1, box_input_len + 1, dtype=tf.float32)\n",
    "\n",
    "                \n",
    "                fractal_dimension = tf.map_fn(\n",
    "                    lambda probability_input: tf.map_fn(\n",
    "                        lambda x: x[0] / x[1],\n",
    "                        elems=(probability_input, probability_numbers),\n",
    "                        fn_output_signature=tf.float32\n",
    "                    ),\n",
    "                    box_input,\n",
    "                    fn_output_signature=tf.float32\n",
    "                )\n",
    "                fractal_dimension = tf.math.reduce_sum(fractal_dimension, axis=1)\n",
    "                \n",
    "                box_outputs.append(fractal_dimension)\n",
    "                \n",
    "            color_distance_outputs.append(box_outputs)\n",
    "            \n",
    "        return color_distance_outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Developing the layer that calculates lacunarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to the article:\n",
    "> LAC is a measure complementary to FD and allows to evaluate how the space of a fractal is filled (Ivanovici & Richard, 2009). From the probability matrix, first and second-order moments are calculated with"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\mu(L) = \\sum_{m=1}^{L^2} mP(m, L)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\mu^2(L) = \\sum_{m=1}^{L^2} m^{2}P(m, L)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> The LAC value for a scale 𝐿 is given by 𝛬(𝐿), which is obtained according to"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\Lambda(L) = \\frac{\\mu^{2}(L) - (\\mu(L))^{2}}{(\\mu(L))^{2}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Lacunarity(tf.keras.layers.Layer):\n",
    "    def __init__(self):\n",
    "        super(Lacunarity, self).__init__()\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        color_distance_outputs = []\n",
    "        \n",
    "        for color_distance_input in inputs:\n",
    "            box_outputs = []\n",
    "            \n",
    "            for box_input in color_distance_input:\n",
    "                probability_numbers = tf.range(1, len(box_input) + 1, dtype=tf.float32)\n",
    "                \n",
    "                mu_first_2 = tf.map_fn(\n",
    "                    lambda x: x[0] * x[1], \n",
    "                    elems=(box_input, probability_numbers),\n",
    "                    fn_output_signature=tf.float32\n",
    "                )\n",
    "                mu_first_2 = tf.math.reduce_sum(mu_first_2, axis=1)\n",
    "                mu_first_2 = tf.math.pow(mu_first_2, 2)\n",
    "                \n",
    "                mu_second = tf.math.pow(probability_numbers, 2)\n",
    "                mu_second = tf.map_fn(\n",
    "                    lambda x: x[0] * x[1], \n",
    "                    elems=(box_input, mu_second),\n",
    "                    fn_output_signature=tf.float32\n",
    "                )\n",
    "                mu_second = tf.math.reduce_sum(mu_second, axis=1)\n",
    "                \n",
    "                lacunarity = tf.math.divide(\n",
    "                    tf.math.subtract(mu_second, mu_first_2),\n",
    "                    mu_first_2\n",
    "                )\n",
    "                \n",
    "                box_outputs.append(lacunarity)\n",
    "                \n",
    "            color_distance_outputs.append(box_outputs)\n",
    "            \n",
    "        return color_distance_outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Developing the layer that calculates percolation C - the average number of clusters per box on a scale L"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to the article:\n",
    "> Let 𝑐<sub>𝑖</sub> be the number of clusters on a box 𝛽<sub>𝑖</sub>, the feature 𝐶(𝐿) that represents the average number of clusters per box on a scale 𝐿 is given by"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "C(L) = \\frac{\\sum_{i=1}^{T(L)} c_{i}}{T(L)}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PercolationC(tf.keras.layers.Layer):\n",
    "    def __init__(self):\n",
    "        super(PercolationC, self).__init__()\n",
    "\n",
    "    def call(self, inputs):\n",
    "        color_distance_outputs = []\n",
    "        \n",
    "        for color_distance_input in inputs:\n",
    "            box_outputs = []\n",
    "            \n",
    "            for box_input in color_distance_input:\n",
    "                percolation_c = tf.math.reduce_mean(\n",
    "                    tf.map_fn(\n",
    "                        lambda batch: tf.map_fn(\n",
    "                            lambda patch: tf.math.reduce_max(tfa.image.connected_components(patch)), \n",
    "                            batch\n",
    "                        ),\n",
    "                        box_input\n",
    "                    ),\n",
    "                    axis=1\n",
    "                )\n",
    "                percolation_c = tf.cast(percolation_c, dtype=tf.float32)\n",
    "                box_outputs.append(percolation_c)\n",
    "                \n",
    "            color_distance_outputs.append(box_outputs)\n",
    "            \n",
    "        return color_distance_outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Developing the layer that calculates percolation M - the average coverage area of the largest cluster on a scale L"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to the article:\n",
    ">Another feature that can be obtained is the average coverage area of the largest cluster in a box and is given by 𝑀(𝐿). Let 𝑚<sub>𝑖</sub> be the size in pixels of the largest cluster of the box 𝛽<sub>𝑖</sub>. The feature 𝑀(𝐿) is givenaccording to"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "M(L) = \\frac{\\sum_{i=1}^{T(L)} \\frac{m_{i}}{L^2}}{T(L)}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PercolationM(tf.keras.layers.Layer):\n",
    "    def __init__(self):\n",
    "        super(PercolationM, self).__init__()\n",
    "\n",
    "    def call(self, inputs):\n",
    "        color_distance_outputs = []\n",
    "        \n",
    "        for color_distance_input in inputs:\n",
    "            box_outputs = []\n",
    "            \n",
    "            for box_input in color_distance_input:\n",
    "                percolation_m = tf.math.reduce_mean(\n",
    "                    tf.map_fn(\n",
    "                        lambda batch: tf.map_fn(\n",
    "                            lambda patch: self.most_common(tf.reshape(tfa.image.connected_components(patch), shape=(-1,))), \n",
    "                            batch\n",
    "                        ),\n",
    "                        box_input\n",
    "                    ),\n",
    "                    axis=1\n",
    "                )\n",
    "                percolation_m = tf.cast(percolation_m, dtype=tf.float32)\n",
    "                box_outputs.append(percolation_m)\n",
    "                \n",
    "            color_distance_outputs.append(box_outputs)\n",
    "            \n",
    "        return color_distance_outputs\n",
    "    \n",
    "    def most_common(self, array):\n",
    "        _, _, counts = tf.unique_with_counts(array)\n",
    "        return tf.math.reduce_max(counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Developing the layer that calculates percolation Q - the average occurrence of percolation on a scale L"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to the article:\n",
    "> We can also verify whether a box 𝛽<sub>𝑖</sub> is percolating. This can be achieved due to a property that states a percolation threshold for different types of structures. In squared matrices (digital images), this threshold has the value of 𝑝 = 0.59275, which means that if the ratio between pixels labelled as 1 and pixels labelled as 0 is greater or equal than 𝑝, the matrix is considered as percolating. Let 𝛺<sub>𝑖</sub> be the number of pixels labelled as 1 in a box 𝛽<sub>𝑖</sub> with size 𝐿 × 𝐿, we determine whether such box is percolating according to"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "q_{i} = \n",
    "\\begin{cases}\n",
    "1, & \\frac{\\Omega_{i}}{L^2} \\ge 0.59275 \\\\\n",
    "0, & \\frac{\\Omega_{i}}{L^2} < 0.59275\n",
    "\\end{cases}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> This results in a binary value for 𝑞<sub>𝑖</sub>, wherein 1 indicates that thebox is percolating. The feature 𝑄(𝐿) regards the average occurrence of percolation on a scale 𝐿 and can be obtained by"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "Q(L) = \\frac{\\sum_{i=1}^{T(L)} q_{i}}{T(L)}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PercolationQ(tf.keras.layers.Layer):\n",
    "    def __init__(self, threshold=0.59275):\n",
    "        super(PercolationQ, self).__init__()\n",
    "        \n",
    "        self.threshold = threshold\n",
    "\n",
    "    def call(self, inputs):\n",
    "        color_distance_outputs = []\n",
    "        \n",
    "        for color_distance_input in inputs:\n",
    "            box_outputs = []\n",
    "            \n",
    "            for box_input in color_distance_input:\n",
    "                number_of_ones_for_every_patch = tf.map_fn(\n",
    "                    lambda batch: tf.map_fn(\n",
    "                        lambda patch: tf.math.reduce_sum(patch),\n",
    "                        batch\n",
    "                    ),\n",
    "                    box_input\n",
    "                )\n",
    "                \n",
    "                box_input_shape = tf.shape(box_input)\n",
    "                _, number_of_patches, box_size, _ = tf.unstack(box_input_shape)\n",
    "                \n",
    "                percolation_q = tf.math.divide(number_of_ones_for_every_patch, box_size ** 2)\n",
    "                percolation_q = tf.math.greater_equal(percolation_q, self.threshold)\n",
    "                percolation_q = tf.cast(percolation_q, dtype=tf.float32)\n",
    "                percolation_q = tf.math.reduce_mean(percolation_q, axis=1)\n",
    "                \n",
    "                box_outputs.append(percolation_q)\n",
    "                \n",
    "            color_distance_outputs.append(box_outputs)\n",
    "            \n",
    "        return color_distance_outputs\n",
    "    \n",
    "    def most_common(self, array):\n",
    "        _, _, counts = tf.unique_with_counts(array)\n",
    "        return tf.math.reduce_max(counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Developing the layer that assembles fractal features into images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to the article:\n",
    "> To serve as input for the incoming CNN classification, the feature vectors generated on the previous layers of the network must be converted into feature matrices. To do so, the 100 features obtained by each distance 𝛥 are rearranged as a 10 × 10 matrix. The matrices generated by 𝛥<sub>ℎ</sub>, 𝛥<sub>𝑒</sub> and 𝛥<sub>𝑚</sub> correspond to the R, G and B colour channels, respectively. ... Since each of the functions 𝐶(𝐿), 𝑄(𝐿), 𝑀(𝐿), 𝛬(𝐿) and 𝐷(𝐿), obtained from a specific 𝛥, generate 20 features, each function is fit exactly into 2 columns of the matrix.\n",
    "\n",
    ">Since each of the functions 𝐶(𝐿), 𝑄(𝐿), 𝑀(𝐿), 𝛬(𝐿) and 𝐷(𝐿), obtained from a specific 𝛥, generate 20 features, each function is fit exactly into 2 columns of the matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AssembleFractalImage(tf.keras.layers.Layer):\n",
    "    def __init__(self):\n",
    "        super(AssembleFractalImage, self).__init__()\n",
    "\n",
    "    def call(self, inputs):\n",
    "        output = tf.convert_to_tensor(inputs)\n",
    "        output = tf.transpose(output, perm=(3, 1, 0, 2))\n",
    "        output = tf.reshape(output, shape=(-1, 10, 10, 3))\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assembling the layers into fractal neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FractalNeuralNetwork(tf.keras.Model):\n",
    "    def __init__(self, class_number, verbose, fractal_resize):\n",
    "        super(FractalNeuralNetwork, self).__init__()\n",
    "        \n",
    "        self.verbose = verbose\n",
    "        \n",
    "        self.box_counting_patches = [BoxCountingPatch(box_size) for box_size in range(3, 41 + 1, 2)]\n",
    "        \n",
    "        self.chebyshev = ChebyshevBinaryPatch()\n",
    "        self.euclidean = EuclideanBinaryPatch()\n",
    "        self.manhattan = ManhattanBinaryPatch()\n",
    "        \n",
    "        self.percolation_c = PercolationC()\n",
    "        self.percolation_m = PercolationM()\n",
    "        self.percolation_q = PercolationQ()\n",
    "        \n",
    "        self.probability = ProbabilityMatrix()\n",
    "        self.fractal_dimension = FractalDimension()\n",
    "        self.lacunarity = Lacunarity()\n",
    "        \n",
    "        self.assemble = AssembleFractalImage()\n",
    "        self.resize = tf.keras.layers.Resizing(width=fractal_resize[0], height=fractal_resize[1])\n",
    "        self.rescale_original = tf.keras.layers.Rescaling(scale=1./255)\n",
    "        self.rescale_fractal = tf.keras.layers.Lambda(lambda x: tf.math.divide(x, tf.math.reduce_max(x)))\n",
    "        \n",
    "        self.mobilenet_v2 = hub.KerasLayer(\"https://tfhub.dev/google/tf2-preview/mobilenet_v2/feature_vector/4\", \n",
    "                                           output_shape=[1280],\n",
    "                                           trainable=False)\n",
    "        self.combine = tf.keras.layers.Add()\n",
    "        self.score = tf.keras.layers.Dense(class_number, activation='softmax')\n",
    "        \n",
    "    def call(self, inputs):        \n",
    "        box_counting_patches = [box_counting_patch(inputs) for box_counting_patch in self.box_counting_patches]\n",
    "        self.log(message='patchify')\n",
    "\n",
    "        chebyshev = self.chebyshev(inputs=box_counting_patches)\n",
    "        self.log(message='chebyshev')\n",
    "        euclidean = self.euclidean(inputs=box_counting_patches)\n",
    "        self.log(message='euclidean')\n",
    "        manhattan = self.manhattan(inputs=box_counting_patches)\n",
    "        self.log(message='manhattan')\n",
    "\n",
    "        percolation_c = self.percolation_c(inputs=[chebyshev, euclidean, manhattan])\n",
    "        self.log(message='percolation_c')\n",
    "        percolation_m = self.percolation_m(inputs=[chebyshev, euclidean, manhattan])\n",
    "        self.log(message='percolation_m')\n",
    "        percolation_q = self.percolation_q(inputs=[chebyshev, euclidean, manhattan])\n",
    "        self.log(message='percolation_q')\n",
    "\n",
    "        probability = self.probability(inputs=[chebyshev, euclidean, manhattan])\n",
    "        self.log(message='probability')\n",
    "\n",
    "        fractal_dimension = self.fractal_dimension(inputs=probability)\n",
    "        self.log(message='fractal_dimension')\n",
    "        lacunarity = self.lacunarity(inputs=probability)\n",
    "        self.log(message='lacunarity')\n",
    "\n",
    "        fractal_output = self.assemble(\n",
    "            inputs=[\n",
    "                fractal_dimension, \n",
    "                lacunarity, \n",
    "                percolation_c, \n",
    "                percolation_m, \n",
    "                percolation_q\n",
    "            ]\n",
    "        )\n",
    "        self.log(message='fractal_output assemble')\n",
    "        fractal_output = self.resize(fractal_output)\n",
    "        self.log(message='fractal_output resize')\n",
    "        fractal_output = self.rescale_fractal(fractal_output)\n",
    "        self.log(message='fractal_output rescale')\n",
    "        fractal_output = self.mobilenet_v2(fractal_output)\n",
    "        self.log(message='fractal_output mobilenet_v2')\n",
    "\n",
    "        original_output = self.rescale_original(inputs)\n",
    "        self.log(message='original_output rescale')\n",
    "        original_output = self.mobilenet_v2(original_output)\n",
    "        self.log(message='original_output mobilenet_v2')\n",
    "\n",
    "        combined_output = self.combine([fractal_output, original_output])\n",
    "        self.log(message='combined_output combine')\n",
    "        output = self.score(combined_output)\n",
    "        self.log(message='output score')\n",
    "        self.log(message='_' * 100)\n",
    "\n",
    "        return output\n",
    "    \n",
    "    def log(self, message):\n",
    "        if self.verbose:\n",
    "            print(f'\\n{message}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 638 images belonging to 2 classes.\n",
      "Found 159 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "generator = tf.keras.preprocessing.image.ImageDataGenerator(\n",
    "    rotation_range=180,\n",
    "    horizontal_flip=True,\n",
    "    vertical_flip=True,\n",
    "    brightness_range=(0.2, 1.5),\n",
    "    validation_split=0.2,\n",
    ")\n",
    "\n",
    "\n",
    "training_set = generator.flow_from_directory(\n",
    "    '/small-data', \n",
    "    target_size=(224, 224), \n",
    "    batch_size=32, \n",
    "    class_mode='categorical', \n",
    "    subset='training'\n",
    ")\n",
    "validation_set = generator.flow_from_directory(\n",
    "    '/small-data', \n",
    "    target_size=(224, 224), \n",
    "    batch_size=32, \n",
    "    class_mode='categorical', \n",
    "    subset='validation'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "CLASS_NUMBER = len(training_set.class_indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data source"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a data source, we use the ISIC Archive."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ISIC Archive is an open source platform with publicly available images of skin lesions under Creative Commons licenses. The images are associated with ground-truth diagnoses and other clinical metadata. Images can be queried using faceted search and downloaded individually or in batches. The initial focus of the archive has been on dermoscopy images of individual skin lesions, as these images are inherently standardized by the use of a specialized acquisition device and devoid of many of the privacy challenges associated with clinical images. To date, the images have been provided by specialized melanoma centers from around the world. The archive is designed to accept contributions from new sources under the Terms of Use and welcomes new contributors. There are ongoing efforts to supplement the dermoscopy images in the archive with close-up clinical images and a broader representation of skin types. The images in the Archive are used to support educational efforts through linkage with Dermoscopedia and are used for Grand Challenges and Live Challenges to engage the computer science community for the development of diagnostic AI."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For more information, go to [ISIC Archive web site](https://www.isic-archive.com/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing TensorFlow callbacks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our convenience, we create a few TensorFlow callbacks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The TensorBoard callback"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to see how the training is going. We add the callback, which will log the metrics to TensorBoard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dir = '../logs/fit/' + datetime.datetime.now().strftime('fractalnet')\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The EarlyStopping callback"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This callback stops training when the metrics (e.g. validation loss) are not improving,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stop_callback = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor=\"val_loss\", \n",
    "    min_delta=0.01, \n",
    "    patience=10, \n",
    "    restore_best_weights=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The ModelCheckpoint callback"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This callback saves the model with the best metrics during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path = 'checkpoints/fractalnet.ckpt'\n",
    "\n",
    "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    checkpoint_path,\n",
    "    monitor='val_loss',\n",
    "    verbose=1,\n",
    "    save_best_only=True,\n",
    "    save_weights_only=False,\n",
    "    save_freq='epoch',\n",
    "    mode='auto'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "patchify\n",
      "\n",
      "chebyshev\n",
      "\n",
      "euclidean\n",
      "\n",
      "manhattan\n",
      "\n",
      "percolation_c\n",
      "\n",
      "percolation_m\n",
      "\n",
      "percolation_q\n",
      "\n",
      "probability\n",
      "\n",
      "fractal_dimension\n",
      "\n",
      "lacunarity\n",
      "\n",
      "fractal_output assemble\n",
      "\n",
      "fractal_output resize\n",
      "\n",
      "fractal_output rescale\n",
      "\n",
      "fractal_output mobilenet_v2\n",
      "\n",
      "original_output rescale\n",
      "\n",
      "original_output mobilenet_v2\n",
      "\n",
      "combined_output combine\n",
      "\n",
      "output score\n",
      "\n",
      "____________________________________________________________________________________________________\n",
      "\n",
      "patchify\n",
      "\n",
      "chebyshev\n",
      "\n",
      "euclidean\n",
      "\n",
      "manhattan\n",
      "\n",
      "percolation_c\n",
      "\n",
      "percolation_m\n",
      "\n",
      "percolation_q\n",
      "\n",
      "probability\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"/usr/local/lib/python3.8/dist-packages/keras/engine/training.py\", line 878, in train_function  *\n        return step_function(self, iterator)\n    File \"/usr/local/lib/python3.8/dist-packages/keras/engine/training.py\", line 867, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/usr/local/lib/python3.8/dist-packages/keras/engine/training.py\", line 860, in run_step  **\n        outputs = model.train_step(data)\n    File \"/usr/local/lib/python3.8/dist-packages/keras/engine/training.py\", line 808, in train_step\n        y_pred = self(x, training=True)\n    File \"/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py\", line 67, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n\n    ValueError: Exception encountered when calling layer \"fractal_neural_network\" (type FractalNeuralNetwork).\n    \n    in user code:\n    \n        File \"<ipython-input-14-4d527705173e>\", line 56, in call  *\n            fractal_dimension = self.fractal_dimension(inputs=probability)\n        File \"/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py\", line 67, in error_handler  **\n            raise e.with_traceback(filtered_tb) from None\n    \n        ValueError: Exception encountered when calling layer \"fractal_dimension\" (type FractalDimension).\n        \n        in user code:\n        \n            File \"<ipython-input-8-9c01012e5742>\", line 13, in call  *\n                _, box_input_len = tf.unstack(box_input_shape)\n        \n            ValueError: Cannot infer argument `num` from shape (None,)\n        \n        \n        Call arguments received:\n          • inputs=[['tf.Tensor(shape=<unknown>, dtype=float32)', 'tf.Tensor(shape=<unknown>, dtype=float32)', 'tf.Tensor(shape=<unknown>, dtype=float32)', 'tf.Tensor(shape=<unknown>, dtype=float32)', 'tf.Tensor(shape=<unknown>, dtype=float32)', 'tf.Tensor(shape=<unknown>, dtype=float32)', 'tf.Tensor(shape=<unknown>, dtype=float32)', 'tf.Tensor(shape=<unknown>, dtype=float32)', 'tf.Tensor(shape=<unknown>, dtype=float32)', 'tf.Tensor(shape=<unknown>, dtype=float32)', 'tf.Tensor(shape=<unknown>, dtype=float32)', 'tf.Tensor(shape=<unknown>, dtype=float32)', 'tf.Tensor(shape=<unknown>, dtype=float32)', 'tf.Tensor(shape=<unknown>, dtype=float32)', 'tf.Tensor(shape=<unknown>, dtype=float32)', 'tf.Tensor(shape=<unknown>, dtype=float32)', 'tf.Tensor(shape=<unknown>, dtype=float32)', 'tf.Tensor(shape=<unknown>, dtype=float32)', 'tf.Tensor(shape=<unknown>, dtype=float32)', 'tf.Tensor(shape=<unknown>, dtype=float32)'], ['tf.Tensor(shape=<unknown>, dtype=float32)', 'tf.Tensor(shape=<unknown>, dtype=float32)', 'tf.Tensor(shape=<unknown>, dtype=float32)', 'tf.Tensor(shape=<unknown>, dtype=float32)', 'tf.Tensor(shape=<unknown>, dtype=float32)', 'tf.Tensor(shape=<unknown>, dtype=float32)', 'tf.Tensor(shape=<unknown>, dtype=float32)', 'tf.Tensor(shape=<unknown>, dtype=float32)', 'tf.Tensor(shape=<unknown>, dtype=float32)', 'tf.Tensor(shape=<unknown>, dtype=float32)', 'tf.Tensor(shape=<unknown>, dtype=float32)', 'tf.Tensor(shape=<unknown>, dtype=float32)', 'tf.Tensor(shape=<unknown>, dtype=float32)', 'tf.Tensor(shape=<unknown>, dtype=float32)', 'tf.Tensor(shape=<unknown>, dtype=float32)', 'tf.Tensor(shape=<unknown>, dtype=float32)', 'tf.Tensor(shape=<unknown>, dtype=float32)', 'tf.Tensor(shape=<unknown>, dtype=float32)', 'tf.Tensor(shape=<unknown>, dtype=float32)', 'tf.Tensor(shape=<unknown>, dtype=float32)'], ['tf.Tensor(shape=<unknown>, dtype=float32)', 'tf.Tensor(shape=<unknown>, dtype=float32)', 'tf.Tensor(shape=<unknown>, dtype=float32)', 'tf.Tensor(shape=<unknown>, dtype=float32)', 'tf.Tensor(shape=<unknown>, dtype=float32)', 'tf.Tensor(shape=<unknown>, dtype=float32)', 'tf.Tensor(shape=<unknown>, dtype=float32)', 'tf.Tensor(shape=<unknown>, dtype=float32)', 'tf.Tensor(shape=<unknown>, dtype=float32)', 'tf.Tensor(shape=<unknown>, dtype=float32)', 'tf.Tensor(shape=<unknown>, dtype=float32)', 'tf.Tensor(shape=<unknown>, dtype=float32)', 'tf.Tensor(shape=<unknown>, dtype=float32)', 'tf.Tensor(shape=<unknown>, dtype=float32)', 'tf.Tensor(shape=<unknown>, dtype=float32)', 'tf.Tensor(shape=<unknown>, dtype=float32)', 'tf.Tensor(shape=<unknown>, dtype=float32)', 'tf.Tensor(shape=<unknown>, dtype=float32)', 'tf.Tensor(shape=<unknown>, dtype=float32)', 'tf.Tensor(shape=<unknown>, dtype=float32)']]\n    \n    \n    Call arguments received:\n      • inputs=tf.Tensor(shape=(None, None, None, None), dtype=float32)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-4440533c086b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m )\n\u001b[1;32m      6\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'adam'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'categorical_crossentropy'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'accuracy'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m model.fit(\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0mtraining_set\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_set\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m       \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mautograph_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1127\u001b[0m           \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint:disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1128\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ag_error_metadata\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1129\u001b[0;31m               \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1130\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1131\u001b[0m               \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    File \"/usr/local/lib/python3.8/dist-packages/keras/engine/training.py\", line 878, in train_function  *\n        return step_function(self, iterator)\n    File \"/usr/local/lib/python3.8/dist-packages/keras/engine/training.py\", line 867, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/usr/local/lib/python3.8/dist-packages/keras/engine/training.py\", line 860, in run_step  **\n        outputs = model.train_step(data)\n    File \"/usr/local/lib/python3.8/dist-packages/keras/engine/training.py\", line 808, in train_step\n        y_pred = self(x, training=True)\n    File \"/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py\", line 67, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n\n    ValueError: Exception encountered when calling layer \"fractal_neural_network\" (type FractalNeuralNetwork).\n    \n    in user code:\n    \n        File \"<ipython-input-14-4d527705173e>\", line 56, in call  *\n            fractal_dimension = self.fractal_dimension(inputs=probability)\n        File \"/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py\", line 67, in error_handler  **\n            raise e.with_traceback(filtered_tb) from None\n    \n        ValueError: Exception encountered when calling layer \"fractal_dimension\" (type FractalDimension).\n        \n        in user code:\n        \n            File \"<ipython-input-8-9c01012e5742>\", line 13, in call  *\n                _, box_input_len = tf.unstack(box_input_shape)\n        \n            ValueError: Cannot infer argument `num` from shape (None,)\n        \n        \n        Call arguments received:\n          • inputs=[['tf.Tensor(shape=<unknown>, dtype=float32)', 'tf.Tensor(shape=<unknown>, dtype=float32)', 'tf.Tensor(shape=<unknown>, dtype=float32)', 'tf.Tensor(shape=<unknown>, dtype=float32)', 'tf.Tensor(shape=<unknown>, dtype=float32)', 'tf.Tensor(shape=<unknown>, dtype=float32)', 'tf.Tensor(shape=<unknown>, dtype=float32)', 'tf.Tensor(shape=<unknown>, dtype=float32)', 'tf.Tensor(shape=<unknown>, dtype=float32)', 'tf.Tensor(shape=<unknown>, dtype=float32)', 'tf.Tensor(shape=<unknown>, dtype=float32)', 'tf.Tensor(shape=<unknown>, dtype=float32)', 'tf.Tensor(shape=<unknown>, dtype=float32)', 'tf.Tensor(shape=<unknown>, dtype=float32)', 'tf.Tensor(shape=<unknown>, dtype=float32)', 'tf.Tensor(shape=<unknown>, dtype=float32)', 'tf.Tensor(shape=<unknown>, dtype=float32)', 'tf.Tensor(shape=<unknown>, dtype=float32)', 'tf.Tensor(shape=<unknown>, dtype=float32)', 'tf.Tensor(shape=<unknown>, dtype=float32)'], ['tf.Tensor(shape=<unknown>, dtype=float32)', 'tf.Tensor(shape=<unknown>, dtype=float32)', 'tf.Tensor(shape=<unknown>, dtype=float32)', 'tf.Tensor(shape=<unknown>, dtype=float32)', 'tf.Tensor(shape=<unknown>, dtype=float32)', 'tf.Tensor(shape=<unknown>, dtype=float32)', 'tf.Tensor(shape=<unknown>, dtype=float32)', 'tf.Tensor(shape=<unknown>, dtype=float32)', 'tf.Tensor(shape=<unknown>, dtype=float32)', 'tf.Tensor(shape=<unknown>, dtype=float32)', 'tf.Tensor(shape=<unknown>, dtype=float32)', 'tf.Tensor(shape=<unknown>, dtype=float32)', 'tf.Tensor(shape=<unknown>, dtype=float32)', 'tf.Tensor(shape=<unknown>, dtype=float32)', 'tf.Tensor(shape=<unknown>, dtype=float32)', 'tf.Tensor(shape=<unknown>, dtype=float32)', 'tf.Tensor(shape=<unknown>, dtype=float32)', 'tf.Tensor(shape=<unknown>, dtype=float32)', 'tf.Tensor(shape=<unknown>, dtype=float32)', 'tf.Tensor(shape=<unknown>, dtype=float32)'], ['tf.Tensor(shape=<unknown>, dtype=float32)', 'tf.Tensor(shape=<unknown>, dtype=float32)', 'tf.Tensor(shape=<unknown>, dtype=float32)', 'tf.Tensor(shape=<unknown>, dtype=float32)', 'tf.Tensor(shape=<unknown>, dtype=float32)', 'tf.Tensor(shape=<unknown>, dtype=float32)', 'tf.Tensor(shape=<unknown>, dtype=float32)', 'tf.Tensor(shape=<unknown>, dtype=float32)', 'tf.Tensor(shape=<unknown>, dtype=float32)', 'tf.Tensor(shape=<unknown>, dtype=float32)', 'tf.Tensor(shape=<unknown>, dtype=float32)', 'tf.Tensor(shape=<unknown>, dtype=float32)', 'tf.Tensor(shape=<unknown>, dtype=float32)', 'tf.Tensor(shape=<unknown>, dtype=float32)', 'tf.Tensor(shape=<unknown>, dtype=float32)', 'tf.Tensor(shape=<unknown>, dtype=float32)', 'tf.Tensor(shape=<unknown>, dtype=float32)', 'tf.Tensor(shape=<unknown>, dtype=float32)', 'tf.Tensor(shape=<unknown>, dtype=float32)', 'tf.Tensor(shape=<unknown>, dtype=float32)']]\n    \n    \n    Call arguments received:\n      • inputs=tf.Tensor(shape=(None, None, None, None), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "model = FractalNeuralNetwork(\n",
    "    class_number=CLASS_NUMBER, \n",
    "    verbose=True,\n",
    "    fractal_resize=(224, 224)\n",
    ")\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model.fit(\n",
    "    training_set, \n",
    "    validation_data=validation_set, \n",
    "    epochs=1, \n",
    "    callbacks=[\n",
    "                tensorboard_callback,\n",
    "                checkpoint_callback,\n",
    "                early_stop_callback\n",
    "            ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load the model with the best metrics (e.g. validation loss) from the checkpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = FractalNeuralNetwork(\n",
    "    class_number=CLASS_NUMBER, \n",
    "    verbose=True,\n",
    "    fractal_resize=(224, 224)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights('./checkpoints/fractalnet.ckpt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_set = generator.flow_from_directory(\n",
    "    '/small-data-test',\n",
    "    target_size=(224, 224),\n",
    "    batch_size=32,\n",
    "    class_mode='categorical'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making diagnoses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_labels = np.concatenate([testing_set[i][1] for i in range(len(testing_set))], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_labels = model.predict(testing_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot the ROC Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr = dict()\n",
    "tpr = dict()\n",
    "auc_metric = dict()\n",
    "\n",
    "diagnosis_index_dict = {v: k for k, v in testing_set.class_indices.items()}\n",
    "\n",
    "for i in range(CLASS_NUMBER):\n",
    "    diagnosis = diagnosis_index_dict[i]\n",
    "    fpr[diagnosis], tpr[diagnosis], _ = roc_curve(true_labels[:, i], predicted_labels[:, i])\n",
    "    auc_metric[diagnosis] = auc(fpr[diagnosis], tpr[diagnosis])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for diagnosis in testing_set.class_indices:\n",
    "    plt.plot(fpr[diagnosis], tpr[diagnosis], label=diagnosis)\n",
    "    \n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver operating characteristic')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Show AUC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auc_metric"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
